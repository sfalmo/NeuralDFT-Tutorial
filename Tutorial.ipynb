{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f906a9-ddd2-4273-9ec0-21922448fde2",
   "metadata": {},
   "source": [
    "# Neural functional theory for inhomogeneous fluids &ndash; Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9883056-640e-4394-9439-2ea7d645731d",
   "metadata": {},
   "source": [
    "This material is available at https://github.com/sfalmo/NeuralDFT-Tutorial and it supplements the following publication:\n",
    "\n",
    "**Why neural functionals suit statistical mechanics**  \n",
    "*Florian Sammüller, Sophie Hermann, and Matthias Schmidt, [J. Phys.: Condens. Matter **36**, 243002](https://doi.org/10.1088/1361-648X/ad326f) (2024); [arXiv:2312.04681](https://arxiv.org/abs/2312.04681).*\n",
    "\n",
    "We show how the physics of simple fluids can be described with **many-body simulations** and **classical density functional theory** (DFT) and how the two approaches can be combined with the help of machine learning to a **neural functional theory**.\n",
    "The connection of the different methods is exemplified for the case of the one-dimensional hard rod fluid with hands-on code examples and exercises in the programming language Julia.\n",
    "Besides the pedagogical overview given here, a more in-depth account of the presented methodology with applications to the hard sphere and Lennard-Jones fluids in three dimensions can be found in\n",
    "\n",
    "**Neural functional theory of inhomogeneous fluids: Fundamentals and applications**  \n",
    "*Florian Sammüller, Sophie Hermann, Daniel de las Heras, and Matthias Schmidt, [Proc. Natl. Acad. Sci. **120**, e2312484120](https://doi.org/10.1073/pnas.2312484120) (2023); [arXiv:2307.04539](https://arxiv.org/abs/2307.04539).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126bd78-8019-44ff-a95c-51fe51674e44",
   "metadata": {},
   "source": [
    "## Technical remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9fc0d-d3f1-472e-a366-3dbf347467fb",
   "metadata": {},
   "source": [
    "Running some of the tasks in this notebook requires a considerable amount of compute power, e.g. for the generation of simulation data and for the training of the neural network.\n",
    "Keep this in mind if you are using an online service with limited resources.\n",
    "For convenience, we also provide ready-to-use data sets and models, which can be downloaded and applied in the relevant parts.\n",
    "\n",
    "Particularly for the machine learning tasks, we recommend using a GPU, as this speeds up training and inference considerably.\n",
    "Using a recent Nvidia GPU should work out of the box.\n",
    "For other GPU vendors, you might need to install the corresponding Julia packages, see https://juliagpu.org/ for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d1d87-381d-4d2c-ad95-4517aa0dbb96",
   "metadata": {},
   "source": [
    "## Part 1: Many-body simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7c825-6a06-42ca-8866-46ad0b316303",
   "metadata": {},
   "source": [
    "Many-body simulations have long become a standard tool for the investigation of classical fluids.\n",
    "Conceptually, they offer a rather straightforward way to predict the behavior of a fluid from a microscopic description, i.e. by specifying the interactions of its constituent particles.\n",
    "However, simulations often come with a significant demand of computational resources.\n",
    "\n",
    "In the following, we focus on Monte Carlo methods for the description of equilibrium systems.\n",
    "Specifically, we choose the grand ensemble where the temperature $T$ and the chemical potential $\\mu$ are kept fixed (we also specify the length $L$ of the one-dimensional domain).\n",
    "The statistical mechanics of such a system is then determined by the equilibrium distribution function $\\psi(x^{(N)}) \\sim \\exp(-\\beta (U(x^{(N)}) - \\mu N))$ where $\\beta = 1 / (k_B T)$ with the Boltzmann constant $k_B$.\n",
    "The potential energy $U(x^{(N)}) = u(x^{(N)}) + \\sum_{i=1}^N V_\\mathrm{ext}(x_i)$ of a given microstate $x^{(N)} = (x_1, x_2, \\dots, x_N)$ of $N$ particles consists of a contribution due to an external potential $V_\\mathrm{ext}(x)$ and of the internal energy $u(x^{(N)})$.\n",
    "If the particles in the fluid possess pairwise interactions, $u(x^{(N)}) = \\sum_{i=1}^N \\sum_{j>i}^N \\phi(|x_j - x_i|)$, where $\\phi(r)$ is the interaction potential for a given distance $r$ of two particles.\n",
    "\n",
    "Monte Carlo is based on the generation of microstates according to their known equilibrium distribution.\n",
    "This is done iteratively by mutating an initial state $A$ into a new state $B$ with a probability such that a given distribution $P$ of states is kept intact.\n",
    "The mutation happens in two stages: a trial transition selects a new state $B$, and a criterion $\\mathrm{acc}(A \\rightarrow B)$ determines if the new state $B$ shall be accepted or if the system shall be reset to the previous state $A$.\n",
    "Specifically, a valid choice for this acceptance probability is the Metropolis criterion $\\mathrm{acc}(A \\rightarrow B) = \\min(1, P(B) / P(A))$.\n",
    "\n",
    "We now apply this scheme to the grand ensemble, which yields the standard grand canonical Monte Carlo (GCMC) method.\n",
    "There are three possible trial transitions: i) a particle is moved to a new position, ii) a particle is inserted, iii) a particle is removed.\n",
    "The goal distribution of states $P$ is the grand canonical equilibrium distribution $\\psi$ (see above).\n",
    "When considering particle displacements, the Metropolis rate becomes\n",
    "$$\n",
    "\\mathrm{acc}(x^{(N)} \\rightarrow \\tilde{x}^{(N)}) = \\min\\left(1, \\exp[-\\beta(U(\\tilde{x}^{(N)}) - U(x^{(N)}))]\\right)\n",
    "$$\n",
    "for moving a particle and changing the initial microstate $x^{(N)}$ to the new configuration $\\tilde{x}^{(N)}$.\n",
    "\n",
    "Let us now illustrate this trial move with code.\n",
    "For simplification, we provide some utilities for common tasks in [simulation.jl](simulation.jl), e.g. `calc_energy` for calculating the potential energy of one particle, `pbc!` for applying periodic boundary conditions, and definitions of containers (structs) which hold the state of the `System` and the `Histograms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e2c09-43f8-4a46-b587-10c70e4a2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"simulation.jl\");\n",
    "\n",
    "function trial_move(system::System; Δxmax=0.1)\n",
    "    if isempty(system.particles)  # If no particles are in the system, do nothing\n",
    "        return\n",
    "    end\n",
    "    i = rand(1:length(system.particles))  # Select random particle with index i\n",
    "    xbefore = system.particles[i]  # Save its initial position\n",
    "    Ebefore = calc_energy(system, i)  # Calculate the initial potential energy of particle i\n",
    "    system.particles[i] += Δxmax * (2 * rand() - 1)  # Move particle to a new position\n",
    "    pbc!(system, i)  # Apply periodic boundary conditions (this places the particle back in the box if it has moved outside of the valid range)\n",
    "    Eafter = calc_energy(system, i)  # Calculate the potential energy of particle i after it has been moved\n",
    "    if rand() > exp(-system.β * (Eafter - Ebefore))\n",
    "        system.particles[i] = xbefore  # Trial move rejected. Reset particle to previous state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0d018-77c0-4807-8321-2b43c9bc4ab3",
   "metadata": {},
   "source": [
    "As the particle number can fluctuate in the grand ensemble, there are additional transitions which add and remove particles at random throughout the simulation.\n",
    "The acceptance probabilities of these transitions can be derived by considering particle exchanges of the system with a virtual reservoir.\n",
    "We spare this derivation here and only give the results\n",
    "$$\n",
    "\\mathrm{acc}(x^{(N)} \\rightarrow x^{(N+1)}) = \\min\\left(1, \\frac{L}{N+1} \\exp[\\beta(\\mu - U(x^{(N+1)}) + U(x^{(N)}))]\\right),\n",
    "$$\n",
    "$$\n",
    "\\mathrm{acc}(x^{(N)} \\rightarrow x^{(N-1)}) = \\min\\left(1, \\frac{N}{L} \\exp[-\\beta(\\mu + U(x^{(N-1)}) - U(x^{(N)}))]\\right).\n",
    "$$\n",
    "\n",
    "These trial transitions are already provided as `trial_insert` and `trial_delete` in [simulation.jl](simulation.jl), feel free to take a look for the implementation details.\n",
    "\n",
    "With all transitions being specified, we can write a simulation loop which consists of an equilibration stage and a stage in which measurements take place.\n",
    "For each simulation step, we perform a sweep over a fixed number of trial transitions which are chosen at random without introducing bias in the insertions and deletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106fd69-45aa-4b93-86cd-4e0ec5263030",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Dates\n",
    "\n",
    "function sweep(system::System; transitions=10, insert_delete_probability=0.2)\n",
    "    for _ in 1:transitions\n",
    "        if rand() < insert_delete_probability  # Randomly select trial transition: either move or insert/delete\n",
    "            rand() < 0.5 ? trial_insert(system) : trial_delete(system)  # Do trial insertions and removals with equal probability\n",
    "        else\n",
    "            trial_move(system)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function simulate(L::Number, μ::Number, T::Number, Vext::Function, ϕ::Function; equilibration_time=Dates.Second(1), production_time=Dates.Second(2), sweep_transitions=10)\n",
    "    system = System(L, μ, T, Vext, ϕ)  # The state of the system is encapsulated in this struct\n",
    "    histograms = Histograms(system)\n",
    "    equilibration_start = now()\n",
    "    while now() - equilibration_start < equilibration_time  # Equilibration stage, no sampling\n",
    "        sweep(system; transitions=sweep_transitions)\n",
    "    end\n",
    "    production_start = now()\n",
    "    while now() - production_start < production_time\n",
    "        sweep(system; transitions=sweep_transitions)\n",
    "        sample(system, histograms)\n",
    "    end\n",
    "    get_results(system, histograms)  # Normalizes histograms and returns (xs, ρ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f09462-9dbf-4a50-b52e-17677bb4e593",
   "metadata": {},
   "source": [
    "Equilibrium averages such as the one-body density profile $\\rho(x) = \\langle \\sum_{i=1}^N \\delta(x - x_i) \\rangle$ are obtained by sampling.\n",
    "For this, the particle configuration is recorded in a position-resolved histogram, which yields the desired average after normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b3637-a940-4b7f-a9b6-8637a89ba318",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample(system::System, histograms::Histograms)\n",
    "    for x in system.particles\n",
    "        bin = ceil(Int, x / L * histograms.bins)  # Calculate the bin index from the given particle position\n",
    "        histograms.ρ[bin] += 1\n",
    "    end\n",
    "    histograms.count += 1  # Needed later for normalization\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87943c55-9b0b-4e0c-830f-c87151d02bc7",
   "metadata": {},
   "source": [
    "Let us now do some simulations to illustrate the usage of the code.\n",
    "We just pass the length $L$ of our system, the thermodynamic statepoint $\\mu$ and $T$, the external potential $V_\\mathrm{ext}(x)$ and the pair interaction potential $\\phi(r)$ to the `simulate` function.\n",
    "In this tutorial, we focus on the hard rod fluid, which is characterized by forbidding the overlap of particles as illustrated below.\n",
    "\n",
    "![Hard rod fluid](figures/hard_rod_fluid.png)\n",
    "\n",
    "This behavior is reflected by the pair interaction potential\n",
    "$$\n",
    "\\phi(r) = \\begin{cases}\n",
    "\\infty, &r < \\sigma,\\\\\n",
    "0, &r \\geq \\sigma,\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\sigma$ specifies the particle size (we set $\\sigma = 1$ in the code).\n",
    "As a simple test case, we choose confinement between hard walls by setting $V_\\mathrm{ext}(x) = \\infty$ at the boundaries of the domain.\n",
    "Change the form of the external potential as well as the rest of the system parameters to explore the behavior of the hard rod fluid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b837897-a9e7-4269-86a4-ae18abf709e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "L = 10.0\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0  # Hard walls at the boundaries\n",
    "ϕ(r) = r < 1.0 ? Inf : 0  # Hard core repulsion\n",
    "\n",
    "xs, ρ = simulate(L, μ, T, Vext, ϕ; equilibration_time=Dates.Second(1), production_time=Dates.Second(2), sweep_transitions=10)\n",
    "\n",
    "plot(xs, ρ, label=\"ρ\", xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cb3b6-554b-43da-a8a8-36c21d5e4520",
   "metadata": {},
   "source": [
    "## Part 2: Classical density functional theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14592a2-c327-4537-a29d-b1a0960ab6a8",
   "metadata": {},
   "source": [
    "Classical DFT is founded on a minimization principle of the grand potential $\\Omega[\\rho]$, which can be expressed as a functional of the one-body density profile $\\rho(x)$.\n",
    "Its strength is therefore the reduction of a many-body problem (see Part 1) to a formally exact description on the level of one-body quantities.\n",
    "By writing out entropic, external and internal contributions of the grand potential, carrying out the functional derivative $\\delta \\Omega[\\rho] / \\delta \\rho(x)$ and demanding that it vanishes at the equilibrium density, one arrives at the Euler-Lagrange equation of DFT,\n",
    "$$\n",
    "c_1(x) = \\ln\\rho(x) + \\beta (V_\\mathrm{ext}(x) - \\mu),\n",
    "$$\n",
    "where $c_1(x)$ is the one-body direct correlation function.\n",
    "This rather abstract object attains a fundamental meaning in classical DFT as it captures the nontrivial effects of the internal interactions within the fluid on the one-body level.\n",
    "Formally, it arises from a functional derivative, \n",
    "$$\n",
    "c_1(x; [\\rho]) = - \\frac{\\delta \\beta F_\\mathrm{exc}[\\rho]}{\\delta \\rho(x)},\n",
    "$$\n",
    "where $F_\\mathrm{exc}[\\rho]$ is the excess free energy, i.e. the part of $\\Omega[\\rho]$ which accounts for the beyond-ideal-gas behavior of the interacting fluid.\n",
    "As an immediate consequence of $F_\\mathrm{exc}[\\rho]$ being a density functional, $c_1(x; [\\rho])$ also attains a functional dependence on $\\rho(x)$, which is made explicit by the bracket notation.\n",
    "In principle, $F_\\mathrm{exc}[\\rho]$ and $c_1(x; [\\rho])$ also depend parametrically on the temperature $T$, but this dependence becomes trivial for the hard rod fluid.\n",
    "\n",
    "By rearranging the above equation to\n",
    "$$\n",
    "\\rho(x) = \\exp(-\\beta(V_\\mathrm{ext}(x) - \\mu) + c_1(x; [\\rho])),\n",
    "$$\n",
    "we can reveal its use in actual DFT applications.\n",
    "Given a suitable expression for $c_1(x; [\\rho])$ (which one has to obtain somehow for a given type of model fluid), a self-consistent iteration scheme can be used to solve for $\\rho(x)$.\n",
    "One such method is the Picard iteration with mixing, in which the iteration is performed as follows:\n",
    "$$\n",
    "\\rho(x) \\leftarrow (1 - \\alpha) \\rho(x) + \\alpha \\rho_\\mathrm{EL}(x).\n",
    "$$\n",
    "Here, $\\alpha$ is a mixing parameter and $\\rho_\\mathrm{EL}(x) = \\exp(-\\beta(V_\\mathrm{ext}(x) - \\mu) + c_1(x; [\\rho]))$ is the right hand side of the rearranged Euler-Lagrange equation.\n",
    "\n",
    "A simple DFT program proceeds as follows:\n",
    "The system parameters $\\mu$ and $T$, an external potential $V_\\mathrm{ext}(x)$ and a functional form of $c_1(x; [\\rho])$ are given, and the density profile is initialized (e.g. with a constant value) on a discrete numerical grid that spans the domain of length $L$.\n",
    "The iteration is then started and Picard steps are performed to update $\\rho(x)$ as described above.\n",
    "If the change of $\\rho(x)$ between iteration steps falls below a predefined numerical tolerance, the iteration is stopped and the converged self-consistent density profile is obtained as a result.\n",
    "We give an example of such a program in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609e6a2-6ff2-43a3-9de0-c2a5375b6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function minimize(L::Number, μ::Number, T::Number, Vext::Function, get_c1::Function; α::Number=0.03, maxiter::Int=10000, dx::Number=0.01, floattype::Type=Float32, tol::Number=max(eps(floattype(1e3)), 1e-8))\n",
    "    L, μ, T = floattype.((L, μ, T))  # Technical detail: we will use Float32 in the machine learning part as neural networks usually operate on single precision floats\n",
    "    xs = collect(floattype, dx/2:dx:L)  # Construct the numerical grid\n",
    "    Vext = Vext.(xs)  # Evaluate the external potential on the grid\n",
    "    infiniteVext = isinf.(Vext)  # Check where Vext is infinite to set ρ = 0 there\n",
    "    ρ, ρEL = zero(xs), zero(xs)  # Preallocate the density profile and an intermediate buffer for iteration\n",
    "    fill!(ρ, 0.5)  # Start with a bulk density of 0.5\n",
    "    c1 = get_c1(xs)  # Obtain the c1 functional for the given numerical grid\n",
    "    i = 0\n",
    "    while true\n",
    "        ρEL .= exp.((μ .- Vext) ./ T .+ c1(ρ))  # Evaluate the RHS of the Euler-Lagrange equation\n",
    "        ρ .= (1 - α) .* ρ .+ α .* ρEL  # Do a Picard iteration step to update ρ\n",
    "        ρ[infiniteVext] .= 0  # Set ρ to 0 where Vext = ∞\n",
    "        clamp!(ρ, 0, Inf)  # Make sure that ρ does not become negative\n",
    "        Δρmax = maximum(abs.(ρ - ρEL)[.!infiniteVext])  # Calculate the remaining discrepancy to check convergence\n",
    "        i += 1\n",
    "        if Δρmax < tol\n",
    "            println(\"Converged (step: $(i), ‖Δρ‖ = $(Δρmax) < $(tol) = tolerance)\")\n",
    "            break  # The remaining discrepancy is below the tolerance: break out of the loop and return the result\n",
    "        end\n",
    "        if !isfinite(Δρmax) || i >= maxiter\n",
    "            println(\"Did not converge (step: $(i) of $(maxiter), ‖Δρ‖: $(Δρmax), tolerance: $(tol))\")\n",
    "            return nothing  # The iteration did not converge, there is no valid result\n",
    "        end\n",
    "    end\n",
    "    xs, ρ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1bcfc2-a09e-492a-bc5d-6b497a7fd7b1",
   "metadata": {},
   "source": [
    "The way in which $c_1(x; [\\rho])$ appears both in the theory as well as in the code example above seems innocuous at first.\n",
    "However, the crux of DFT is finding a suitable functional expression for $c_1(x; [\\rho])$ (or equivalently for $F_\\mathrm{exc}[\\rho]$) for a given fluid model, and much of the ongoing research deals exactly with this problem.\n",
    "In Part 3, we will show how to efficiently use neural networks to capture such a nontrivial functional mapping from simulation data.\n",
    "\n",
    "In the following, we proceed analytically as the focus lies on the hard rod system, which is at present one of very few model fluids where the exact excess free energy functional could be found.\n",
    "This success can be traced back to the purely geometrical nature of this particular system.\n",
    "For hard spheres in 3D, fundamental measure theory provides a similar geometric approach to the construction of excess functionals, although the results remain approximate in this more general geometry.\n",
    "\n",
    "For the exact hard rod density functional, which is attributed to Jerry Percus, the excess free energy can be expressed as \n",
    "$$\n",
    "\\beta F_\\mathrm{exc}[\\rho] = \\int \\mathrm{d}x \\Phi(n_0(x; [\\rho]), n_1(x; [\\rho]))\n",
    "$$\n",
    "where the free energy density has the specific form $\\Phi(n_0, n_1) = - n_0 \\ln(1 - n_1)$.\n",
    "The functions $n_0(x; [\\rho])$ and $n_1(x; [\\rho])$ are *weighted* densities which arise from convolutions of the density profile with the weight functions $\\omega_0(x) = (\\delta(x-R) + \\delta(x+R)) / 2$ and $\\omega_1(x) = \\Theta(R - |x|)$, i.e. $n_\\alpha(x; [\\rho]) = (\\omega_\\alpha \\star \\rho)(x)$.\n",
    "From this excess free energy functional, one can easily obtain by functional differentiation the result\n",
    "$$\n",
    "c_1(x; [\\rho]) = - \\sum_{\\alpha=0,1} \\left(\\omega_\\alpha \\star \\frac{\\partial \\Phi}{\\partial n_\\alpha}\\right)(x)\n",
    "$$\n",
    "for the one-body direct correlation function.\n",
    "\n",
    "We implement a method that constructs the Percus $c_1(x; [\\rho])$ for a given numerical grid in the following.\n",
    "As assistance, we provide `conv_fft` to evaluate convolutions efficiently in Fourier space and `get_weights_Percus` to obtain $\\omega_\\alpha(x)$ on the numerical grid in the file [dft.jl](dft.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f54d39-1a71-4bfe-aea5-381f6b7f0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"dft.jl\")\n",
    "\n",
    "function get_c1_Percus(xs)\n",
    "    ω0, ω1 = get_weights_Percus(xs)\n",
    "    conv(f, g) = conv_fft(f, g; dx=xs[2]-xs[1])\n",
    "    function (ρ)\n",
    "        n0, n1 = conv(ω0, ρ), conv(ω1, ρ)  # Calculate the weighted densities\n",
    "        ∂ϕ_∂n0 = -log.(1 .- n1)  # Evaluate the partial derivatives of the excess free energy density\n",
    "        ∂ϕ_∂n1 = n0 ./ (1 .- n1)\n",
    "        -(conv(ω0, ∂ϕ_∂n0) .+ conv(ω1, ∂ϕ_∂n1))  # Return c1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a01d4-ac2d-459b-8f25-91bcf39b69c1",
   "metadata": {},
   "source": [
    "Now we can perform some DFT minimizations to see if the results match the simulations.\n",
    "Note that in general, you might have to adjust the mixing parameter $\\alpha$ to ensure convergence of the Picard iteration (see the keyword arguments of `minimize`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9629b4-a355-4485-a492-cfce12ed0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10.0\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0\n",
    "\n",
    "xs, ρ = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, ρ, label=\"ρ\", xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e58a0-e8b9-4ed4-8b2e-ac99a5074609",
   "metadata": {},
   "source": [
    "By comparing simulation and DFT results for the same choices of $L$, $\\mu$, $T$ and $V_\\mathrm{ext}(x)$, we can cross-check the validity of the density profiles of the hard rod fluid.\n",
    "However, it is obvious that both methods have substantial (and quite contrary) restrictions:\n",
    "- The simulation data is noisy, which can only be improved with longer simulation runs. This quickly becomes prohibitively expensive, in particular if one wishes to perform many individual simulations to explore the behavior for different system parameters. However, one is free to change the type of considered fluid by simply modifying the form of the internal interactions.\n",
    "- The DFT calculation is fast and does not suffer from noisy results, thus enabling vast and efficient parameter studies. The results are exact for the case of hard rods, but as illustrated above, one had to find and implement a suitable density functional in order to capture the nontrivial intrinsic correlations. For more complex fluids, there is little hope in deriving an exact functional analytically, and the construction of good approximations often proves to be very difficult.\n",
    "\n",
    "In the final part of this tutorial, we will combine the advantages of both methods with the help of machine learning.\n",
    "We will demonstrate how to acquire an accurate and flexible representation of $c_1(x; [\\rho])$ by training a neural network with simulation data.\n",
    "The resulting *neural functional* can be used in the minimization scheme as seen above, but it can also reveal more fundamental information about the statistical mechanics of the considered fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b885ad7-9bba-409a-9069-a595801b8568",
   "metadata": {},
   "source": [
    "## Part 3: Neural functional theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7f3c7-fbf2-4fcf-a01b-07c3f4461651",
   "metadata": {},
   "source": [
    "![test](figures/neural_functional_workflow.png)\n",
    "\n",
    "The survey in Part 2 indicates that classical DFT is a powerful concept for the determination of fluid equilibra, but that it is limited by the difficulty of finding analytic expressions for $c_1(x; [\\rho])$.\n",
    "In the following, we will train a neural network with simulation data (see Part 1) to obtain a representation of $c_1(x; [\\rho])$.\n",
    "We will then show applications of this neural functional, which include the prediction of density profiles and the investigation of neural functional calculus.\n",
    "\n",
    "To generate a suitable data set, simulations of systems with randomized inhomogeneous external potentials $V_\\mathrm{ext}(x)$ and random values of the chemical potential $\\mu$ are performed.\n",
    "For the construction of an appropriate form of $V_\\mathrm{ext}(x)$, we combine Fourier modes, linear segments and hard walls, which are defined in the following.\n",
    "We also write a method which generates a combination of these elementary functions with randomly chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ec0a2-b2e9-4af2-ac4f-076f74ae7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vext_sin(x; n::Int, A::Number, φ::Number, L::Number) = A * sin(2π * x * n / L + φ)\n",
    "\n",
    "Vext_lin(x; x1::Number, x2::Number, E1::Number, E2::Number) = x > x1 && x < x2 ? E1 + (x - x1) * (E2 - E1) / (x2 - x1) : 0\n",
    "\n",
    "Vext_wall(x; xw::Number, L::Number) = x < xw || x > L - xw ? Inf : 0\n",
    "\n",
    "function generate_Vext(L::Number; num_sin=4, num_lin=rand(1:5), wall=true)\n",
    "    Avar = 1.0\n",
    "    sin_parameters = []\n",
    "    for n in 1:num_sin  # Generate random parameters for periodic sine functions with increasing frequency\n",
    "        push!(sin_parameters, (n = n, A = randn() * Avar, φ = rand() * 2π, L = L))\n",
    "    end\n",
    "    Evar = 1.0\n",
    "    lin_parameters = []\n",
    "    for _ in 1:num_lin  # Generate random parameters for discontinuous linear segments\n",
    "        push!(lin_parameters, (x1 = round(rand() * L, digits=2), x2 = round(rand() * L, digits=2), E1 = randn() * Evar, E2 = randn() * Evar))\n",
    "    end\n",
    "    xwmax = 1.0\n",
    "    wall_params = (xw = round(rand() * xwmax, digits=2), L = L)  # Set a random wall width\n",
    "    function (x)  # Return a method which evaluates a combination of all functions with the chosen parameters above\n",
    "        result = 0.0\n",
    "        for sin_params in sin_parameters\n",
    "            result += Vext_sin(x; sin_params...)\n",
    "        end\n",
    "        for lin_params in lin_parameters\n",
    "            result += Vext_lin(x; lin_params...)\n",
    "        end\n",
    "        if wall\n",
    "            result += Vext_wall(x; wall_params...)\n",
    "        end\n",
    "        result\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77eed93-1c98-4e8a-8d1a-d16dc1d60a24",
   "metadata": {},
   "source": [
    "To get an idea of the randomized inhomogeneous environments, let us generate and plot some $V_\\mathrm{ext}(x)$ profiles.\n",
    "Run the following code cell a few times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3d5c0-4343-4bd1-92fa-232aa2311648",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10.0\n",
    "dx = 0.01\n",
    "xs = dx/2:dx:L\n",
    "Vext_generated = generate_Vext(L)\n",
    "plot(xs, Vext_generated.(xs), label=\"Vext\", xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20481ad5-30ac-4a47-90dc-2d623d6dd255",
   "metadata": {},
   "source": [
    "We can now proceed to generate reference data by running simulations with these randomized external potentials and random values of the chemical potential as input.\n",
    "Of course, this requires spending some computational resources.\n",
    "You can run the simulations yourself, but be aware that this does take a considerable amount of compute time (~hours) if you want to get good data.\n",
    "For this, change `use_prepared_simulations` to `false` in the following code cell and set an appropriate number of simulations and the time to spend for the equilibration and production stage.\n",
    "Alternatively, you could \"fake\" the simulations by using the exact Percus DFT as illustrated in Part 2.\n",
    "\n",
    "For readers without the required computational resources and/or patience, we have prepared a ready-to-use data set for the following machine learning tasks, which is downloaded by default as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a9508-ee8e-4219-bd2c-c08c51b1bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles, Downloads\n",
    "\n",
    "use_prepared_simulations = true  # Choose if you want to use pregenerated simulations (true) or if you want to generate a simulation data set from scratch (false)\n",
    "\n",
    "if use_prepared_simulations\n",
    "    println(\"Downloading pregenerated simulation data set\")\n",
    "    \n",
    "    Downloads.download(\"https://www.staff.uni-bayreuth.de/~bt306964/neuraldft-tutorial/data.tar\", \"data.tar\")\n",
    "    run(`tar xf data.tar`)  # You should now have a directory \"data\" with a bunch of simulation files\n",
    "    datadir = \"data\"\n",
    "else\n",
    "    num_sim = 512\n",
    "    equilibration_time = Dates.Second(10)\n",
    "    production_time = Dates.Second(200)\n",
    "    nthreads = Threads.nthreads()\n",
    "    println(\"Generating simulation reference data from scratch. This will take approximately $(canonicalize(num_sim * (equilibration_time + production_time) / nthreads)).\")\n",
    "    \n",
    "    L = 10\n",
    "    ϕ(r) = r < 1.0 ? Inf : 0  # Hard core repulsion\n",
    "    μmin, μmax = -7.0, 5.0\n",
    "    T = 1.0\n",
    "    \n",
    "    datadir = mkdir(\"data_$(now())\")\n",
    "    println(\"Saving results to $(datadir)\")\n",
    "    Threads.@threads for i in 1:num_sim\n",
    "        μ = μmin + rand() * (μmax - μmin)\n",
    "        Vext_generated = generate_Vext(L)\n",
    "        println(\"Simulation $(i) running...\")\n",
    "        xs, ρ = simulate(L, μ, T, Vext_generated, ϕ; equilibration_time, production_time)\n",
    "        μloc = μ .- Vext_generated.(xs)\n",
    "        writedlm(\"$(datadir)/$(i).dat\", [xs μloc ρ])\n",
    "        println(\"Simulation $(i) done\")\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"There are $(length(readdir(datadir))) result files in the directory $(datadir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88961f1-24b5-4cbf-b70a-2a292c1b875a",
   "metadata": {},
   "source": [
    "If you have generated your own training set, the results are now located in the directory `data_<timestamp>` (use the file explorer of JupyterLab on the left side).\n",
    "Otherwise, proceed with the pregenerated simulations which are located in `data` after successful download and unpacking.\n",
    "\n",
    "Let us first plot some of the data files to check if they are reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c3412-8a4a-4bf1-94f0-8c238f337bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data\"  # ... or use your own simulation results from above\n",
    "\n",
    "sim = rand(readdir(datadir, join=true))  # Select a random simulation within the data directory\n",
    "xs, μloc, ρ = eachcol(readdlm(sim))  # Read the result file and split columns\n",
    "display(plot(xs, μloc, label=\"μloc\", xlims=(0, L), xlabel=\"x/σ\"))\n",
    "display(plot(xs, ρ, label=\"ρ\", xlims=(0, L), xlabel=\"x/σ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36a173-95b2-43af-a205-2270cedbd512",
   "metadata": {},
   "source": [
    "Note that besides $\\rho(x)$, we have saved the local chemical potential $\\mu_\\mathrm{loc}(x) = \\mu - V_\\mathrm{ext}(x)$ as a further one-body field.\n",
    "This additional information suffices to calculate the one-body direct correlation function $c_1(x) = \\ln \\rho(x) - \\beta \\mu_\\mathrm{loc}(x)$ for each $x$ where $\\rho(x) \\neq 0$, which is the target of our following investigation.\n",
    "\n",
    "In Part 2, we have shown that the one-body direct correlation function cannot only be obtained pointwise by the above relation, but that it also constitutes a universal functional $c_1(x; [\\rho])$ of the density profile, without having to invoke the local chemical potential $\\mu_\\mathrm{loc}(x)$ explicitly.\n",
    "In the following, we attempt to machine-learn this functional relationship with a neural network.\n",
    "For constructing the specific input-output mapping, we appeal to some physical background.\n",
    "Formally, the functional dependence on $\\rho(x)$ is given with respect to the profile of the whole system.\n",
    "However, for short-ranged pair interactions, the influence of the surrounding density profile on the value of $c_1(x; [\\rho])$ at a certain position $x$ also remains very short-ranged.\n",
    "Therefore, we can adopt a *local* learning strategy: the density profile is taken only within a narrow window around a given position as input to the neural network, which is trained to yield the *scalar value* of $c_1(x; [\\rho])$ at that position.\n",
    "The whole one-body direct correlation profile can be recovered by evaluating the neural network at different positions across the system domain.\n",
    "The following schematic illustrates this setup.\n",
    "\n",
    "![Neural network](figures/neural_network.png)\n",
    "\n",
    "Due to this specific construction, the data has to be prepared accordingly prior to the training, i.e. the input-output pairs of $\\rho$-windows and $c_1$-values have to be generated from the full profiles.\n",
    "For this, some utility functions are given in the file [neural.jl](neural.jl).\n",
    "Note that we choose a window width of $1 \\sigma$ to each side.\n",
    "For the hard rod fluid, we know that this choice suffices, as the exact analytic solution for $c_1(x; [\\rho])$ also has a convolutional range of $1 \\sigma$.\n",
    "For other fluid models, one would have to determine the window width heuristically, for which the neural functional calculus (see below) could provide some guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fed85c-c836-4b9b-a4b7-0fd1dbe8f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"neural.jl\")\n",
    "\n",
    "ρ_profiles, c1_profiles = read_sim_data(datadir)\n",
    "ρ_windows, c1_values = generate_inout(ρ_profiles, c1_profiles; window_width=1.0, dx=xs[2]-xs[1])  # window width = 1σ suffices for hard rods\n",
    "\n",
    "size(ρ_windows), size(c1_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccd6cf-9295-4bb3-9f1a-c4dcbe78a97a",
   "metadata": {},
   "source": [
    "The data is now ready to be given to a neural network for training.\n",
    "We choose a simple architecture with fully-connected hidden layers and continuous activation functions (e.g. `softplus`).\n",
    "For the construction of the neural network and the subsequent machine learning routines, the framework [Flux.jl](https://fluxml.ai/) is used.\n",
    "We also specify a standard optimizer and give the generated input-output pairs to a data loader, which automates shuffling and batching during training.\n",
    "As this is a common regression task, we select the mean squared error as the loss function and the mean absolute error as the metric.\n",
    "The learning rate is decreased exponentially after the first few epochs, which improves the final training result.\n",
    "\n",
    "You can choose in the following code cell via `use_pretrained_model` whether you want to download a ready-to-use model or whether you want to do the training yourself.\n",
    "Running the training on a GPU is recommended.\n",
    "Otherwise, the code runs as-is on the CPU, but training might be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5943f3-c73e-4ee3-a48b-11df758c17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON, Dates, Downloads, Flux, Printf\n",
    "using CUDA  # For Nvidia GPU support, defaults to CPU if no CUDA device is available. Switch out this package if you have a different GPU manufacturer.\n",
    "\n",
    "use_pretrained_model = true  # Choose if you want to use the pretrained model (true) or if you want to do the training yourself (false)\n",
    "\n",
    "if use_pretrained_model\n",
    "    println(\"Downloading pretrained model\")\n",
    "    \n",
    "    Downloads.download(\"https://www.staff.uni-bayreuth.de/~bt306964/neuraldft-tutorial/model.bson\", \"model.bson\")\n",
    "    BSON.@load \"model.bson\" model\n",
    "else\n",
    "    println(\"Training model from scratch\")\n",
    "    ρ_windows, c1_values = (ρ_windows, c1_values) |> gpu\n",
    "    \n",
    "    model = Chain(\n",
    "        Dense(size(ρ_windows)[1] => 128, softplus),\n",
    "        Dense(128 => 64, softplus),\n",
    "        Dense(64 => 32, softplus),\n",
    "        Dense(32 => 1)\n",
    "    ) |> gpu\n",
    "    \n",
    "    display(model)  # Show a summary of the model with the number of fittable parameters\n",
    "    \n",
    "    opt = Flux.setup(Adam(), model)  # Set up a standard Adam optimizer\n",
    "    \n",
    "    loader = Flux.DataLoader((ρ_windows, c1_values), batchsize=256, shuffle=true)  # Initialize the DataLoader to yield shuffled batches\n",
    "    \n",
    "    loss(m, x, y) = Flux.mse(m(x), y)  # Use mean squared error as loss\n",
    "    metric(m, x, y) = Flux.mae(m(x), y)  # Use mean absolute error as metric\n",
    "    \n",
    "    get_learning_rate(epoch; initial=0.0001, rate=0.03, wait=5) = epoch < wait ? initial : initial * (1 - rate)^(epoch - wait)\n",
    "\n",
    "    model_savefile = \"model_$(now()).bson\"\n",
    "    println(\"Saving model to $(model_savefile)\")\n",
    "    for epoch in 1:250  # Do the training loop\n",
    "        learning_rate = get_learning_rate(epoch)\n",
    "        Flux.adjust!(opt, learning_rate)\n",
    "        @printf \"Epoch: %3i (learning_rate: %.2e)...\" epoch learning_rate; flush(stdout)\n",
    "        Flux.train!(loss, model, loader, opt)\n",
    "        @printf \" loss: %.5f, metric: %.5f\\n\" loss(model, ρ_windows, c1_values) metric(model, ρ_windows, c1_values); flush(stdout)\n",
    "        model = model |> cpu\n",
    "        BSON.@save model_savefile model\n",
    "        model = model |> gpu\n",
    "    end\n",
    "end\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbdbc6-dc0e-4b7b-b097-b1b184a772ba",
   "metadata": {},
   "source": [
    "The model is now ready to be used for inference.\n",
    "For convenience, we write a method that lets us evaluate the whole $c_1$-profile via the trained model for a given density profile.\n",
    "This is necessary due to the local nature of the mapping, which prevents $\\rho(x)$ from being used directly as input.\n",
    "Instead, as shown above, the density profile must be restructured into appropriate windows which are passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77be17-38a2-43e8-b983-01a9218c5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_c1_neural(model, xs)\n",
    "    window_bins = length(model.layers[1].weight[1,:])  # Get the number of input bins from the shape of the first layer\n",
    "    model = model |> gpu\n",
    "    function (ρ)\n",
    "        ρ_windows = generate_windows(ρ; window_bins) |> gpu  # The helper function generate_windows is defined in neural.jl\n",
    "        model(ρ_windows) |> cpu |> vec  # Evaluate the model, make sure the result gets back to the CPU, and transpose it to a vector\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794784ed-6f48-4041-a857-e4758f339982",
   "metadata": {},
   "source": [
    "The central use of the neural functional is revealed by applying it for the self-consistent calculation of density profiles.\n",
    "Just like we have used the analytic functional due to Percus in Part 2, we can now employ the neural functional to obtain results for $c_1(x; [\\rho])$ within a DFT minimization.\n",
    "Importantly, the specific minimization scheme to update and iterate $\\rho(x)$ until convergence remains unchanged upon switching out the analytic one-body direct correlation functional with its neural counterpart.\n",
    "Hence, this *neural* DFT is an effective means to circumvent the cumbersome search for analytic functionals while retaining the merits of the DFT minimization procedure.\n",
    "\n",
    "We show in the following that neural DFT indeed yields accurate results for the considered hard rod fluid.\n",
    "Exact reference data for comparison can be obtained immediately by the Percus theory (one would have to resort to simulation results for other fluid models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a13f3f-0a98-4e6b-b3a2-3ab921e4d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10.0\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0\n",
    "\n",
    "xs, ρ_neural = minimize(L, μ, T, Vext, xs -> get_c1_neural(model, xs))\n",
    "xs, ρ_Percus = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, [ρ_neural ρ_Percus], label=[\"ρ (neural)\" \"ρ (Percus)\"], linestyle=[:solid :dash], xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d43e0-c55e-4b0e-8f23-ddf6c25dab42",
   "metadata": {},
   "source": [
    "Although the box size has been kept constant during training and in the above example, we can immediately apply the neural functional for different choices of $L$ due to its local nature.\n",
    "This also gives us the possibility to run some rather demanding quality checks.\n",
    "For narrow confinement, which has clearly not been included in our training data, the hard rod fluid yields very interesting density profiles.\n",
    "The following test reveals that the neural functional is able to extrapolate to these unseen cases and that the results from neural DFT match the analytic theory with surprising accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27243c56-f7b7-44ca-bc0c-ff3e1041a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 3.0  # Try other values between 2.0 and 4.0, the density profiles will be interesting\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0\n",
    "\n",
    "xs, ρ_neural = minimize(L, μ, T, Vext, xs -> get_c1_neural(model, xs))\n",
    "xs, ρ_Percus = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, [ρ_neural ρ_Percus], label=[\"ρ (neural)\" \"ρ (Percus)\"], linestyle=[:solid :dash], xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182361d-f3fb-4dec-aca2-3c0d6d2d34db",
   "metadata": {},
   "source": [
    "We can also increase $L$ significantly compared to the previous cases.\n",
    "The ability of the neural functional to be used straightforwardly in this manner creates the opportunity of efficient multiscale investigations.\n",
    "In the following, this is illustrated by considering the sedimentation of the hard rod fluid in a column with hard walls at the top and at the bottom.\n",
    "The external potential is chosen to slowly increase linearly with height $x$ in order to model the effect of gravity.\n",
    "Within the sedimentation column, the density closely follows the equation of state of the hard rod fluid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343c6db-8c21-4491-9038-9d49d848b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 100.0  # You can try to increase L even further\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0.05 * x  # Hard walls at the boundaries and linearly increasing within the sedimentation column\n",
    "\n",
    "xs, ρ_neural = minimize(L, μ, T, Vext, xs -> get_c1_neural(model, xs))\n",
    "xs, ρ_Percus = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, [ρ_neural ρ_Percus], label=[\"ρ (neural)\" \"ρ (Percus)\"], linestyle=[:solid :dash], xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2352da7-65db-471d-a910-545ca48bc610",
   "metadata": {},
   "source": [
    "Besides using the trained model directly in DFT minimizations, we show in the following that one can extract much more fundamental physical information from the neural functional.\n",
    "For this, we aim at implementing functional calculus on the basis of the neural network.\n",
    "An important prerequisite for this is to understand the concept of automatic differentiation.\n",
    "In fact, we have used autodifferentiation already above to realize the adaptation of the neural network parameters during its training.\n",
    "This has been hidden within the `Flux.train!` method, which evaluates for the given training data the output of the neural network as well as gradients of the loss function with respect to all trainable parameters.\n",
    "These gradients are used to trace the error of the neural network predictions back to the weights in order to nudge them in the \"right direction\", i.e. to decrease the loss.\n",
    "\n",
    "Reverse mode automatic differentiation computes derivatives by recording individual subexpressions of a given composite function, e.g. of the neural network, during evaluation.\n",
    "The chain rule is then applied programmatically to yield the derivative of the function in a second computational pass.\n",
    "As this mechanism is central to machine learning, many frameworks come with ready-to-use implementations.\n",
    "For instance, Flux.jl provides the `gradient` method, which can be used as follows for scalar-valued functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7929-21b4-4c2f-824c-c4c71661d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x, y) = x^2 + sin(x * y)  # Some arbitrary test function\n",
    "df(x, y) = (2*x + y * cos(x * y), x * cos(x * y))  # Analytic derivative to test autodiff results\n",
    "\n",
    "x, y = π, 2.0  # Values for which the derivative will be computed\n",
    "\n",
    "println(\"analytic: \", df(x, y))\n",
    "println(\"autodiff: \", Flux.gradient(f, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85ea55-833d-4134-b286-e920770ad897",
   "metadata": {},
   "source": [
    "Similarly, we can derive the output of the neural functional with respect to the input density via autodifferentiation.\n",
    "But what do we get out of this?\n",
    "\n",
    "In Part 2, we have seen that the one-body direct correlation functional is obtained from the functional derivative of the excess free energy, $c_1(x; [\\rho]) = -\\delta \\beta F_\\mathrm{exc}[\\rho] / \\delta \\rho(x)$, and that it is a central object in DFT calculations.\n",
    "Applying an additional functional derivative yields another important quantity, the two-body direct correlation function \n",
    "$$\n",
    "c_2(x, x'; [\\rho]) = \\frac{\\delta c_1(x; [\\rho])}{\\delta \\rho(x')}.\n",
    "$$\n",
    "In fact, a complete hierarchy of direct correlation functions $c_n(x, x', ...; [\\rho])$ is defined by iterating subsequent functional derivatives.\n",
    "\n",
    "From fundamental physical considerations, further relations can be obtained for these direct correlation functions.\n",
    "In particular, by applying Noether's theorem to statistical mechanical systems, one arrives at so-called sum rules, which constrain the interrelation of different quantities of interest.\n",
    "One such sum rule which connects $c_1(x)$, $c_2(x, x')$ and $\\rho(x)$ reads as follows:\n",
    "$$\n",
    "\\nabla c_1(x) = \\int \\mathrm{d}x' c_2(x, x') \\nabla' \\rho(x').\n",
    "$$\n",
    "As these identities emerge from fundamental invariances of equilibrium many-body systems, they can be used as further valuable tests to check the consistency of the neural functional.\n",
    "\n",
    "We implement in the following a method to calculate $c_2(x, x'; [\\rho])$ via autodifferentiation of a given function for $c_1(x; [\\rho])$.\n",
    "Note that autodifferentiation can be used very generically, which gives us a way to obtain $c_2(x, x'; [\\rho])$ also from the Percus $c_1(x; [\\rho])$ without having to derive and implement the analytic expression by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae80ea5-ab40-41be-83df-7f0dd6ec7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_c2_autodiff(c1_function, xs)\n",
    "    dx = xs[2] - xs[1]\n",
    "    function (ρ)\n",
    "        Flux.jacobian(c1_function, ρ)[1] / dx\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbb52c-b91a-4ede-baa1-c387b69298c6",
   "metadata": {},
   "source": [
    "First, we run some quick tests to illustrate the usage of this method.\n",
    "As test density input, we take profiles that follow from a randomized continuous external potential.\n",
    "Run the cell a few times to see different results for $c_2(x, x'; [\\rho])$.\n",
    "Also try to switch out the neural correlation functional for the Percus expression, the use of autodifferentiation remains straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9160f3-cb34-4f63-a404-c2be8e2556ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10.0\n",
    "μ, T = 0.0, 1.0\n",
    "\n",
    "xs, ρ = minimize(L, μ, T, generate_Vext(L; num_sin=5, num_lin=0, wall=false), get_c1_Percus)\n",
    "\n",
    "c1_func = get_c1_neural(model, xs)\n",
    "# c1_func = get_c1_Percus(xs)  # The Percus c1 can also be used in the following autodifferentiation, no need for a pen-and-paper derivation :)\n",
    "\n",
    "c2_func = get_c2_autodiff(c1_func, xs)\n",
    "\n",
    "c1 = c1_func(ρ)\n",
    "c2 = c2_func(ρ)\n",
    "\n",
    "display(plot(xs, ρ, label=\"ρ\", xlims=(0, L), xlabel=\"x/σ\"))\n",
    "display(plot(xs, c1, label=\"c1\", xlims=(0, L), xlabel=\"x/σ\"))\n",
    "heatmap(xs, xs, c2, colorbar_title=\"c2\", aspect_ratio=1, xlims=(0, L), ylims=(0, L), xlabel=\"x/σ\", ylabel=\"x'/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19e9dd-d363-48e4-86d3-3187a5b2a23f",
   "metadata": {},
   "source": [
    "Now we can check the above Noether sum rule.\n",
    "The gradients are evaluated numerically on the spatial grid via finite differences, for which we have supplied the function `finite_diff` in [neural.jl](neural.jl).\n",
    "The following plot shows the right and left hand side of the identity, which coincide when evaluating the quantities with a neural functional that has been trained sufficiently well.\n",
    "Recall that we have neither specified $c_2(x, x'; [\\rho])$ nor the Noether identity during training.\n",
    "The successful reproduction of such fundamental results implies that the neural functional does not merely interpolate values of $c_1(x; [\\rho])$, but that it instead also captures the essential physics of the underlying many-body problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea1349-40ac-4308-a078-1fcd24897d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = xs[2] - xs[1]\n",
    "grad_c1 = finite_diff(c1; dx)\n",
    "grad_ρ = finite_diff(ρ; dx)\n",
    "lhs = grad_c1  # Left hand side of the Noether sum rule\n",
    "rhs = c2 * grad_ρ * dx  # Right hand side of the Noether sum rule\n",
    "plot(xs, [lhs rhs], label=[\"∇c1(x)\" \"∫dx' c2(x,x') ∇'ρ(x')\"], linestyle=[:solid :dash], xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc1c45-d708-4760-9788-d6937dfc0d52",
   "metadata": {},
   "source": [
    "To complete the functional calculus, we now consider the *inverse* operation of functional differentiation.\n",
    "This gives us direct access to the excess free energy $F_\\mathrm{exc}[\\rho]$, as can be deduced from the definition of $c_1(x; [\\rho])$ via its functional derivative.\n",
    "Formally, functional line integration serves as such an inverse operation, and it amounts to performing a line integral in function space.\n",
    "By choosing a linear parameterization $\\rho_a(x) = a \\rho(x)$, one can derive the explicit result\n",
    "$$\n",
    "\\beta F_\\mathrm{exc}[\\rho] = - \\int \\mathrm{d}x \\rho(x) \\int_0^1 \\mathrm{d}a c_1(x; [\\rho_a]).\n",
    "$$\n",
    "\n",
    "On the level of the neural functional (or, in fact, any representation of the one-body direct correlation functional), this expression is easy to evaluate numerically by discretizing both integrals.\n",
    "We write a method for this in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab99b01-739a-4b5f-8f38-db0ddb645c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_Fexc_funcintegral(c1_function, xs; num_a=100)\n",
    "    dx = xs[2] - xs[1]\n",
    "    da = 1 / num_a\n",
    "    as = da/2:da:1\n",
    "    function (ρ)\n",
    "        aintegral = zero(ρ)\n",
    "        for a in as\n",
    "            aintegral .+= c1_function(a .* ρ)\n",
    "        end\n",
    "        -sum(ρ .* aintegral) * dx * da\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b57164-90a3-4bef-b471-ec35ae5a9af0",
   "metadata": {},
   "source": [
    "In the following code cell, randomized inhomogeneous systems are considered as above to test the excess free energy calculation via functional line integration.\n",
    "We additionally implement the exact analytic Percus expression for $F_\\mathrm{exc}[\\rho]$ (see Part 2) in order to judge the results.\n",
    "The values of $F_\\mathrm{exc}[\\rho]$ as obtained by functional line integration of the neural correlation functional show almost no discrepancy to the exact analytic theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af931ca-2218-425d-8de2-50598c4e2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10.0\n",
    "μ, T = 0.0, 1.0\n",
    "\n",
    "xs, ρ = minimize(L, μ, T, generate_Vext(L), get_c1_Percus)\n",
    "\n",
    "c1_func = get_c1_neural(model, xs)\n",
    "# c1_func = get_c1_Percus(xs)\n",
    "\n",
    "Fexc_func = get_Fexc_funcintegral(c1_func, xs)\n",
    "\n",
    "function get_Fexc_Percus(xs)\n",
    "    dx = xs[2] - xs[1]\n",
    "    ω0, ω1 = get_weights_Percus(xs)\n",
    "    conv(f, g) = conv_fft(f, g; dx)\n",
    "    function (ρ)\n",
    "        n0, n1 = conv(ω0, ρ), conv(ω1, ρ)\n",
    "        ϕ = n0 .* log.(1 .- n1)\n",
    "        -sum(ϕ) * dx\n",
    "    end\n",
    "end\n",
    "\n",
    "Fexc_Percus = get_Fexc_Percus(xs)\n",
    "\n",
    "println(\"Fexc from analytic expression (Percus):       \", Fexc_Percus(ρ))\n",
    "println(\"Fexc from functional line integral (neural):  \", Fexc_func(ρ))\n",
    "plot(xs, ρ, label=\"ρ\", xlims=(0, L), xlabel=\"x/σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4601fd3-4f25-430a-8314-7d36d5234b94",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5086a3a-6040-41d0-bfc3-ca53c27238c8",
   "metadata": {},
   "source": [
    "In this tutorial, we have investigated various methods for the description of classical statistical many-body systems.\n",
    "In particular, we have focused on developing a machine learning framework based on classical DFT and many-body simulations, which combines the strengths of both methods and which facilitates to overcome their individual limitations.\n",
    "\n",
    "As preparation, the fundamentals of GCMC simulations were laid out in Part 1.\n",
    "We have shown that many-body simulations are suitable to obtain equilibrium averages such as the density profile from the microscopic description of a system.\n",
    "This was exemplified specifically for the hard rod fluid in inhomogeneous environments.\n",
    "However, the computational demands of simulations often turn out to be too restrictive in concrete applications.\n",
    "\n",
    "In Part 2, classical DFT was presented as a coarse-grained method that operates directly on the level of the one-body density rather than on the full many-body picture, which greatly reduces the computational complexity of the problem.\n",
    "For the case of the hard rod fluid, we have reproduced the exact analytic density functional due to Percus and have shown its application in a standard DFT minimization for the self-consistent calculation of density profiles.\n",
    "While DFT is formally exact for arbitrary fluid models, we have emphasized the difficulty of finding analytic functionals and the exceptionality of the hard rod result.\n",
    "These considerations led us to the application of machine learning with the goal of capturing functional maps from simulation data.\n",
    "\n",
    "Part 3 gives a detailed account of our machine learning procedure, in which a neural network serves as the central object for the representation of a functional relationship.\n",
    "Training data has been acquired with GCMC simulations, and we have described in this regard how one can generate suitable external potentials randomly to cover diverse inhomogeneous environments.\n",
    "The actual training procedure was based on the functional mapping from the density profile $\\rho(x)$ to the one-body direct correlation functional $c_1(x; [\\rho])$, which were hence taken respectively as input and target output of the neural network.\n",
    "Crucially, however, we have adopted a *local* learning strategy.\n",
    "The neural network was constructed to yield only the scalar value of $c_1(x; [\\rho])$ at a certain location $x$ when provided with the surrounding density profile within a narrow window around $x$ as input.\n",
    "This setup imposes no substantial restriction, as the whole profile of $c_1(x; [\\rho])$ can be recovered by evaluating the neural network (in parallel) at multiple positions within the system domain.\n",
    "However, there are numerous benefits to considering the functional relationship locally, such as an improvement of training statistics, the prescription of the short-ranged nature of $c_1(x; [\\rho])$ and the applicability of the neural functional to multiscale problems.\n",
    "\n",
    "We have then investigated applications of the trained neural correlation functional and its use in DFT minimizations.\n",
    "The results of this *neural* DFT were highly accurate even for unseen external environments, which could be demonstrated for the hard rod fluid by comparison to the exact Percus solution.\n",
    "Further, we have shown the implementation of functional calculus on the basis of the neural network, which enabled us to obtain related quantities such as the two-body direct correlation functional $c_2(x, x'; [\\rho])$ or the excess free energy $F_\\mathrm{exc}[\\rho]$.\n",
    "In this regard, automatic differentiation was utilized for the evaluation of functional derivatives, and a functional line integral served as the explicit inverse operation.\n",
    "The results of the neural functional calculus could be verified by the exact Percus theory.\n",
    "A special case, which has not been covered in this tutorial, is the application of the neural functional calculus to systems with spatially constant density, where important bulk properties such as the equation of state and the bulk pair structure can be determined.\n",
    "It is a valuable excercise to use the above tools for the investigation of these bulk results.\n",
    "Further details are given in our research paper ([doi:10.1073/pnas.2312484120](https://doi.org/10.1073/pnas.2312484120), [arXiv:2307.04539](https://arxiv.org/abs/2307.04539)).\n",
    "\n",
    "As a further consistency check of the neural functional, we have shown the validity of a sum rule which arises from the application of Noether's theorem to statistical many-body systems, and which can hence be traced back to a fundamental invariance property of phase space.\n",
    "It is therefore nontrivial that the neural network reproduces the identity given that no such information was provided during training.\n",
    "There are many more sum rules which result from thermal Noether invariance and which are described in more detail in the accompanying manuscript ([arXiv:2312.04681](https://arxiv.org/abs/2312.04681)).\n",
    "We encourage the reader to implement and verify them; the above code provides the required utilities and may serve as guidance.\n",
    "\n",
    "Even though we have focused on the hard rod fluid, which is owed to the pedagogical purpose of this tutorial and to the availability of exact theoretical results, the presented machine learning framework is generically applicable to other fluid models with short-ranged interactions.\n",
    "You can try this yourself by modifying the above code examples.\n",
    "Simulation data can be generated for a different type of fluid by simply changing the form of the pair potential $\\phi(r)$ and by selecting appropriate parameters for the random generation of chemical and external potentials.\n",
    "The training routine of the neural functional then proceeds identically, provided that the input range of the density is chosen sufficiently large for the specific interaction type (this could be checked e.g. by evaluating $c_2(x, x'; [\\rho])$ via autodifferentiation).\n",
    "The trained neural functional can be applied as before in DFT minimizations and in the functional calculus, which is particularly relevant for fluid models where no satisfactory analytic functionals exist.\n",
    "One could even go beyond an isothermal setting and include variations of the temperature, which then serves as a further input parameter to the neural correlation functional.\n",
    "In total, the neural functional theory leaves much room for further research and we have demonstrated here that it serves as a promising hybrid approach to the investigation of inhomogeneous fluids."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
