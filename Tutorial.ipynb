{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f906a9-ddd2-4273-9ec0-21922448fde2",
   "metadata": {},
   "source": [
    "# Neural functional theory for inhomogeneous fluids &ndash; Tutorial\n",
    "\n",
    "This material provides a pedagogical introduction to the concepts and methods presented in\n",
    "\n",
    "**Neural functional theory of inhomogeneous fluids: Fundamentals and applications**  \n",
    "Florian Sammüller, Sophie Hermann, Daniel de las Heras, and Matthias Schmidt (2023); [arXiv:2307.04539](https://arxiv.org/abs/2307.04539).\n",
    "\n",
    "We show how the physics of simple fluids can be described with **many-body simulations** and **classical density functional theory** (DFT) and how the two approaches can be combined with the help of machine learning to a **neural functional theory**.\n",
    "Compared to the above publication, we take a step back here and consider the one-dimensional hard rod fluid.\n",
    "For this simple model, we demonstrate the central ideas of the different methods and provide hands-on code examples and exercises in the programming language Julia.\n",
    "Besides the theoretical overview given here, a more in-depth account of the physical and methodological background can be found in the accompanying manuscript:\n",
    "\n",
    "**Why neural functionals suit statistical mechanics**  \n",
    "Florian Sammüller, Sophie Hermann, and Matthias Schmidt (2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126bd78-8019-44ff-a95c-51fe51674e44",
   "metadata": {},
   "source": [
    "## Part 0: Technical remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9fc0d-d3f1-472e-a366-3dbf347467fb",
   "metadata": {},
   "source": [
    "Running some of the tasks in this notebook requires a considerable amount of compute power, e.g. the generation of simulation data and the training of the neural network.\n",
    "Keep this in mind if you are using an online service with limited resources.\n",
    "\n",
    "Particularly for the machine learning tasks, we recommend using a GPU, as this speeds up training and inference considerably.\n",
    "If you run this notebook locally and have a GPU available, open a Julia shell in a new tab, install the required packages and restart the Julia kernel of this notebook.\n",
    "Further details are given in https://juliagpu.org/ where you can find the right packages for different GPU vendors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539a674-57df-4fc5-a5fd-ccf8e3d54090",
   "metadata": {},
   "source": [
    "### Setup for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266fbae-9201-46bb-9041-80aded10e8a9",
   "metadata": {},
   "source": [
    "*Skip this part if you are not running this notebook in Google Colab*\n",
    "\n",
    "In Google Colab, you can use a machine with an Nvidia GPU for free, but you first have to install Julia and the required packages.\n",
    "Follow these steps each time you start a new runtime:\n",
    "1. Connect to a GPU runtime:\n",
    "   - In the Google Colab interface, click the arrow (&#9660;) next to *Connect* and select *Change runtime type*\n",
    "   - Under *Hardware accelerator*, select some option with \"GPU\" in its name\n",
    "   - Save the runtime settings and click *Connect*\n",
    "3. Run the script below and wait a few minutes for the Julia install to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05d4a3-9ee6-4f48-ba99-e1d5afa601f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "set -e\n",
    "\n",
    "JULIA_VERSION=\"1.9.2\"\n",
    "\n",
    "if [ -z `which julia` ]; then\n",
    "    JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
    "    echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
    "    BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
    "    URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
    "    wget -nv $URL -O /tmp/julia.tar.gz\n",
    "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
    "    rm /tmp/julia.tar.gz\n",
    "    \n",
    "    echo \"Installing IJulia package...\"\n",
    "    julia -e 'using Pkg; pkg\"add IJulia; precompile;\"' &> /dev/null\n",
    "    \n",
    "    echo \"Installing IJulia kernel...\"\n",
    "    julia -e 'using IJulia; IJulia.installkernel(\"julia\")'\n",
    "    KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
    "    KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
    "    mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia\n",
    "fi\n",
    "\n",
    "git clone https://github.com/sfalmo/NeuralDFT-Tutorial.git\n",
    "cp NeuralDFT-Tutorial/*.jl NeuralDFT-Tutorial/*.toml .\n",
    "\n",
    "echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebb1f5-5e46-4743-8dc0-a2e4495a23f1",
   "metadata": {},
   "source": [
    "3. Switch to the Julia kernel:\n",
    "   - Click the arrow (&#9660;) next to *Connect* and select *Change runtime type*\n",
    "   - Under *Runtime type*, select the option \"julia 1.9.2\" (do not change the *Hardware accelerator* which you have chosen in step 1)\n",
    "5. Install the required Julia packages by running the following code cell, which will also take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca46517-0783-43cf-ba69-9fda5df3b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "\n",
    "Pkg.activate(\".\")\n",
    "Pkg.add(\"CUDA\")\n",
    "Pkg.add(\"cuDNN\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228d9d2-34f8-45f7-976d-fc6e7156c6ba",
   "metadata": {},
   "source": [
    "You are now ready to start the tutorial.\n",
    "In Part 3, GPU support can simply be enabled with `using CUDA`; all required packages have just been installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d1d87-381d-4d2c-ad95-4517aa0dbb96",
   "metadata": {},
   "source": [
    "## Part 1: Many-body simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7c825-6a06-42ca-8866-46ad0b316303",
   "metadata": {},
   "source": [
    "Many-body simulations have long become a standard tool for the investigation of classical fluids.\n",
    "Conceptually, they offer a rather straightforward way to predict the behavior of a fluid from a microscopic description, i.e. by specifying the interactions of its constituent particles.\n",
    "However, simulations often come with a significant demand of computational resources.\n",
    "\n",
    "In the following, we focus on Monte Carlo methods for the description of equilibrium systems.\n",
    "Specifically, we choose the grand ensemble where the temperature $T$ and the chemical potential $\\mu$ are kept fixed (we also specify the length $L$ of the one-dimensional domain).\n",
    "The statistical mechanics of such a system is then determined by the equilibrium distribution function $\\psi(x^{(N)}) \\sim \\exp(-\\beta (U(x^{(N)}) - \\mu N))$ where $\\beta = 1 / (k_B T)$ with the Boltzmann constant $k_B$.\n",
    "The potential energy $U(x^{(N)}) = u(x^{(N)}) + \\sum_{i=1}^N V_\\mathrm{ext}(x_i)$ of a given microstate $x^{(N)} = (x_1, x_2, \\dots, x_N)$ of $N$ particles consists of a contribution due to an external potential $V_\\mathrm{ext}(x)$ and of the internal energy $u(x^{(N)})$.\n",
    "If the particles in the fluid possess pairwise interactions, $u(x^{(N)}) = \\sum_{i=1}^N \\sum_{j>i}^N \\phi(|x_j - x_i|)$, where $\\phi(r)$ is the interaction potential for a given distance $r$ of two particles.\n",
    "\n",
    "Monte Carlo is based on the generation of microstates according to their known equilibrium distribution.\n",
    "This is done iteratively by mutating an initial state $A$ into a new state $B$ with a probability such that a given distribution $P$ of states is kept intact.\n",
    "The mutation happens in two stages: a trial transition selects a new state $B$, and a criterion $\\mathrm{acc}(A \\rightarrow B)$ determines if the new state $B$ shall be accepted or if the system shall be reset to the previous state $A$.\n",
    "Specifically, a valid choice for this acceptance probability is the Metropolis criterion $\\mathrm{acc}(A \\rightarrow B) = \\min(1, P(B) / P(A))$.\n",
    "\n",
    "We now apply this scheme to the grand ensemble, which yields the standard grand canonical Monte Carlo (GCMC) method.\n",
    "There are three possible trial transitions: i) a particle is moved to a new position, ii) a particle is inserted, iii) a particle is removed.\n",
    "The goal distribution of states $P$ is the grand canonical equilibrium distribution $\\psi$ (see above).\n",
    "When considering particle displacements, the Metropolis rate becomes\n",
    "$$\n",
    "\\mathrm{acc}(x^{(N)} \\rightarrow \\tilde{x}^{(N)}) = \\min\\left(1, \\exp[-\\beta(U(\\tilde{x}^{(N)}) - U(x^{(N)}))]\\right)\n",
    "$$\n",
    "for moving a particle and changing the initial microstate $x^{(N)}$ to the new configuration $\\tilde{x}^{(N)}$.\n",
    "\n",
    "Let us now illustrate this trial move with code.\n",
    "For simplification, we provide some utilities for common tasks in simulation.jl, e.g. `calc_particle_interaction` for the calculation of the energy, `pbc!` for applying periodic boundary conditions, and definitions of containers (structs) which hold the state of the `System` and the `Histograms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e2c09-43f8-4a46-b587-10c70e4a2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"simulation.jl\");\n",
    "\n",
    "function trial_move(system::System; Δxmax=0.1)\n",
    "    if isempty(system.particles)  # If no particles are in the system, do nothing\n",
    "        return\n",
    "    end\n",
    "    i = rand(1:length(system.particles))  # Select random particle with index i\n",
    "    xbefore = system.particles[i]  # Save its initial position\n",
    "    Ebefore = calc_particle_interaction(system, i)  # Calculate the initial potential energy of particle i\n",
    "    system.particles[i] += Δxmax * (2 * rand() - 1)  # Move particle to a new position\n",
    "    pbc!(system, i)  # Apply periodic boundary conditions (this places the particle back in the box if it has moved outside of the valid range)\n",
    "    Eafter = calc_particle_interaction(system, i)  # Calculate the potential energy of particle i after it has been moved\n",
    "    if rand() > exp(-system.β * (Eafter - Ebefore))\n",
    "        system.particles[i] = xbefore  # Trial move rejected. Reset particle to previous state\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0d018-77c0-4807-8321-2b43c9bc4ab3",
   "metadata": {},
   "source": [
    "As the particle number can fluctuate in the grand ensemble, there are additional transitions which add and remove particles at random throughout the simulation.\n",
    "The acceptance probabilities of these transitions can be derived by considering particle exchanges of the system with a virtual reservoir.\n",
    "We spare this derivation here and only give the results\n",
    "$$\n",
    "\\mathrm{acc}(x^{(N)} \\rightarrow x^{(N+1)}) = \\min\\left(1, \\frac{L}{N+1} \\exp[\\beta(\\mu - U(x^{(N+1)}) + U(x^{(N)}))]\\right),\n",
    "$$\n",
    "$$\n",
    "\\mathrm{acc}(x^{(N)} \\rightarrow x^{(N-1)}) = \\min\\left(1, \\frac{N}{L} \\exp[-\\beta(\\mu + U(x^{(N-1)}) - U(x^{(N)}))]\\right).\n",
    "$$\n",
    "\n",
    "These trial transitions are already provided as `trial_insert` and `trial_delete` in simulation.jl, feel free to take a look for the implementation details.\n",
    "\n",
    "With all transitions being specified, we can write a simulation loop which consists of an equilibration stage and a stage in which measurements take place.\n",
    "For each simulation step, we perform a sweep over a fixed number of trial transitions which are chosen at random without any bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106fd69-45aa-4b93-86cd-4e0ec5263030",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Dates\n",
    "\n",
    "function sweep(system::System; transitions=10, insert_delete_probability=0.2)\n",
    "    for _ in 1:transitions\n",
    "        if rand() < insert_delete_probability  # Randomly select trial transition: either move or insert/delete\n",
    "            rand() < 0.5 ? trial_insert(system) : trial_delete(system)  # Do trial insertions and removals with equal probability\n",
    "        else\n",
    "            trial_move(system)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function simulate(L::Number, μ::Number, T::Number, Vext::Function, ϕ::Function; equilibration_time=Dates.Second(1), production_time=Dates.Second(2), sweep_transitions=10)\n",
    "    system = System(L, μ, T, Vext, ϕ)  # The state of the system is encapsulated in this struct\n",
    "    histograms = Histograms(system)\n",
    "    equilibration_start = now()\n",
    "    while now() - equilibration_start < equilibration_time  # Equilibration stage, no sampling\n",
    "        sweep(system; transitions=sweep_transitions)\n",
    "    end\n",
    "    production_start = now()\n",
    "    while now() - production_start < production_time\n",
    "        sweep(system; transitions=sweep_transitions)\n",
    "        sample(system, histograms)\n",
    "    end\n",
    "    get_results(system, histograms)  # Normalizes histograms and returns (xs, ρ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f09462-9dbf-4a50-b52e-17677bb4e593",
   "metadata": {},
   "source": [
    "Equilibrium averages such as the one-body density profile $\\rho(x) = \\langle \\sum_{i=1}^N \\delta(x - x_i) \\rangle$ are obtained by sampling.\n",
    "For this, the particle configuration is recorded in a position-resolved histogram, which yields the desired average after normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b3637-a940-4b7f-a9b6-8637a89ba318",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample(system::System, histograms::Histograms)\n",
    "    for x in system.particles\n",
    "        bin = ceil(Int, x / L * histograms.bins)  # Calculate the bin index from the given particle position\n",
    "        histograms.ρ[bin] += 1\n",
    "    end\n",
    "    histograms.count += 1  # Needed later for normalization\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87943c55-9b0b-4e0c-830f-c87151d02bc7",
   "metadata": {},
   "source": [
    "Let us now do some simulations of the hard rod fluid to illustrate the usage of the code.\n",
    "We just pass the length $L$ of our system, the thermodynamic statepoint $\\mu$ and $T$, the external potential $V_\\mathrm{ext}(x)$ and the pair interaction potential $\\phi(r)$ to the `simulate` function.\n",
    "As a simple test case, we choose confinement between hard walls by setting $V_\\mathrm{ext} = \\infty$ at the boundaries of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b837897-a9e7-4269-86a4-ae18abf709e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 10.0\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0  # Hard walls at the boundaries\n",
    "ϕ(r) = r < 1.0 ? Inf : 0  # Hard core repulsion\n",
    "\n",
    "xs, ρ = simulate(L, μ, T, Vext, ϕ; equilibration_time=Dates.Second(1), production_time=Dates.Second(2), sweep_transitions=10)\n",
    "\n",
    "plot(xs, ρ, label=\"ρ\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cb3b6-554b-43da-a8a8-36c21d5e4520",
   "metadata": {},
   "source": [
    "## Part 2: Classical density functional theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14592a2-c327-4537-a29d-b1a0960ab6a8",
   "metadata": {},
   "source": [
    "Classical DFT is founded on a minimization principle of the grand potential $\\Omega[\\rho]$, which can be expressed as a functional of the one-body density profile $\\rho(x)$.\n",
    "Its strength is therefore the reduction of a many-body problem (see Part 1) to a formally exact description on the level of one-body quantities.\n",
    "By writing out entropic, external and internal contributions of the grand potential, carrying out the functional derivative $\\delta \\Omega[\\rho] / \\delta \\rho(x)$ and demanding that it vanishes at the equilibrium density, one arrives at the Euler-Lagrange equation of DFT,\n",
    "$$\n",
    "c_1(x) = \\ln\\rho(x) + \\beta (V_\\mathrm{ext}(x) - \\mu),\n",
    "$$\n",
    "where $c_1(x)$ is the one-body direct correlation function.\n",
    "This rather abstract object attains a fundamental meaning in classical DFT as it captures the nontrivial effects of the internal interactions within the fluid on the one-body level.\n",
    "Formally, it arises from a functional derivative, $c_1(x) = - \\delta \\beta F_\\mathrm{exc}[\\rho] / \\delta \\rho(x)$, where $F_\\mathrm{exc}[\\rho]$ is the excess (i.e. beyond ideal gas) free energy.\n",
    "As an immediate consequence of $F_\\mathrm{exc}[\\rho]$ being a density functional, $c_1(x; [\\rho])$ also has a functional dependence on $\\rho(x)$, which is made explicit by the bracket notation.\n",
    "In principle, $F_\\mathrm{exc}[\\rho]$ and $c_1(x; [\\rho])$ also depend parametrically on the temperature $T$, but this dependence becomes trivial for the hard rod fluid.\n",
    "\n",
    "By rearranging the above equation to\n",
    "$$\n",
    "\\rho(x) = \\exp(-\\beta(V_\\mathrm{ext}(x) - \\mu) + c_1(x; [\\rho])),\n",
    "$$\n",
    "we can reveal its use in actual DFT applications.\n",
    "Given a suitable expression for $c_1(x; [\\rho])$ (which one has to obtain somehow for a given type of model fluid), a self-consistent iteration scheme can be used to solve for $\\rho(x)$.\n",
    "One such method is the Picard iteration with mixing, in which the iteration is performed as follows:\n",
    "$$\n",
    "\\rho(x) \\leftarrow (1 - \\alpha) \\rho(x) + \\alpha \\rho_\\mathrm{EL}(x).\n",
    "$$\n",
    "Here, $\\alpha$ is a mixing parameter and $\\rho_\\mathrm{EL}(x) = \\exp(-\\beta(V_\\mathrm{ext}(x) - \\mu) + c_1(x; [\\rho]))$ is the right hand side of the rearranged Euler-Lagrange equation.\n",
    "\n",
    "A simple DFT program proceeds as follows:\n",
    "The system parameters $\\mu$ and $T$, an external potential $V_\\mathrm{ext}(x)$ and a functional form of $c_1(x; [\\rho])$ are given, and the density profile is initialized (e.g. with a constant value) on a discrete numerical grid that spans the domain of length $L$.\n",
    "The iteration is then started and Picard steps are performed to update $\\rho(x)$ as described above.\n",
    "If the change of $\\rho(x)$ between iteration steps falls below a predefined numerical tolerance, the iteration is stopped and the converged self-consistent density profile is obtained as a result.\n",
    "We give an example of such a program in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609e6a2-6ff2-43a3-9de0-c2a5375b6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function minimize(L::Number, μ::Number, T::Number, Vext::Function, get_c1::Function; α::Number=0.05, maxiter::Int=10000, dx::Number=0.01, floattype::Type=Float32, tol::Number=max(eps(floattype(1e3)), 1e-8))\n",
    "    L, μ, T = floattype.((L, μ, T))  # Technical detail: we will use Float32 in the machine learning part as neural networks usually operate on single precision floats\n",
    "    xs = collect(floattype, dx/2:dx:L)  # Construct the numerical grid\n",
    "    Vext = Vext.(xs)  # Evaluate the external potential on the grid\n",
    "    infiniteVext = isinf.(Vext)  # Check where Vext is infinite to set ρ = 0 there\n",
    "    ρ, ρEL = zero(xs), zero(xs)  # Preallocate the density profile and an intermediate buffer for iteration\n",
    "    fill!(ρ, 0.5)  # Start with a bulk density of 0.5\n",
    "    c1 = get_c1(xs)  # Obtain the c1 functional for the given numerical grid\n",
    "    i = 0\n",
    "    while true\n",
    "        ρEL .= exp.((μ .- Vext) ./ T .+ c1(ρ))  # Evaluate the RHS of the Euler-Lagrange equation\n",
    "        ρ .= (1 - α) .* ρ .+ α .* ρEL  # Do a Picard iteration step to update ρ\n",
    "        ρ[infiniteVext] .= 0  # Set ρ to 0 where Vext = ∞\n",
    "        clamp!(ρ, 0, Inf)  # Make sure that ρ does not become negative\n",
    "        Δρmax = maximum(abs.(ρ - ρEL)[.!infiniteVext])  # Calculate the remaining discrepancy to check convergence\n",
    "        i += 1\n",
    "        if Δρmax < tol\n",
    "            println(\"Converged (step: $(i), ‖Δρ‖ = $(Δρmax) < $(tol) = tolerance)\")\n",
    "            break  # The remaining discrepancy is below the tolerance: break out of the loop and return the result\n",
    "        end\n",
    "        if !isfinite(Δρmax) || i >= maxiter\n",
    "            println(\"Did not converge (step: $(i) of $(maxiter), ‖Δρ‖: $(Δρmax), tolerance: $(tol))\")\n",
    "            return nothing  # The iteration did not converge, there is no valid result\n",
    "        end\n",
    "    end\n",
    "    xs, ρ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1bcfc2-a09e-492a-bc5d-6b497a7fd7b1",
   "metadata": {},
   "source": [
    "The way in which $c_1(x; [\\rho])$ appears both in the theory as well as in the code example above seems innocuous at first.\n",
    "However, the crux of DFT is finding a suitable functional expression for $c_1(x; [\\rho])$ (or equivalently for $F_\\mathrm{exc}[\\rho]$) for a given fluid model, and much of the ongoing research deals exactly with this problem.\n",
    "In Part 3, we will show how to efficiently use neural networks to capture such a nontrivial functional mapping from simulation data.\n",
    "\n",
    "In the following, we proceed analytically as the focus lies on the hard rod system, which is the only model fluid at present where the exact excess free energy functional could be found.\n",
    "This success can be traced back to the purely geometrical nature of this particular system.\n",
    "For hard spheres in 3D, fundamental measure theory provides a similar geometric approach to the construction of excess functionals, although the results are no longer exact.\n",
    "\n",
    "\n",
    "Returning to the exact hard rod result due to Percus, the excess free energy can be expressed as \n",
    "$$\n",
    "\\beta F_\\mathrm{exc}[\\rho] = \\int \\mathrm{d}x \\Phi(n_0(x; [\\rho]), n_1(x; [\\rho]))\n",
    "$$\n",
    "where the free energy density has the form $\\Phi(n_0, n_1) = - n_0 \\ln(1 - n_1)$.\n",
    "The functions $n_0(x; [\\rho])$ and $n_1(x; [\\rho])$ are *weighted* densities which arise from convolutions of the density profile with the weight functions $\\omega_0(x) = (\\delta(x-R) + \\delta(x+R)) / 2$ and $\\omega_1(x) = \\Theta(R - |x|)$, i.e. $n_\\alpha(x; [\\rho]) = (\\omega_\\alpha \\star \\rho)(x)$.\n",
    "From this excess free energy functional, one can easily obtain by functional differentiation the result\n",
    "$$\n",
    "c_1(x; [\\rho]) = - \\sum_{\\alpha=0,1} \\left(\\omega_\\alpha \\star \\frac{\\partial \\Phi}{\\partial n_\\alpha}\\right)(x)\n",
    "$$\n",
    "for the one-body direct correlation function.\n",
    "\n",
    "We implement a method that constructs the Percus $c_1(x; [\\rho])$ for a given numerical grid in the following.\n",
    "As assistance, we provide `conv_fft` to evaluate convolutions efficiently in Fourier space and `get_weights_Percus` to obtain $\\omega_\\alpha(x)$ on the numerical grid in the file dft.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f54d39-1a71-4bfe-aea5-381f6b7f0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"dft.jl\")\n",
    "\n",
    "function get_c1_Percus(xs)\n",
    "    ω0, ω1 = get_weights_Percus(xs)\n",
    "    conv(f, g) = conv_fft(f, g; dx=xs[2]-xs[1])\n",
    "    function (ρ)\n",
    "        n0, n1 = conv(ω0, ρ), conv(ω1, ρ)\n",
    "        ∂ϕ_∂n0 = -log.(1 .- n1)\n",
    "        ∂ϕ_∂n1 = n0 ./ (1 .- n1)\n",
    "        -(conv(ω0, ∂ϕ_∂n0) .+ conv(ω1, ∂ϕ_∂n1))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a01d4-ac2d-459b-8f25-91bcf39b69c1",
   "metadata": {},
   "source": [
    "Now we can perform some DFT minimizations to see if the results match the simulations.\n",
    "Note that in general, you might have to adjust the mixing parameter $\\alpha$ to ensure convergence of the Picard iteration (see the keyword arguments of `minimize`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9629b4-a355-4485-a492-cfce12ed0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 10.0\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0\n",
    "\n",
    "xs, ρ = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, ρ, label=\"ρ\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e58a0-e8b9-4ed4-8b2e-ac99a5074609",
   "metadata": {},
   "source": [
    "As we can check by comparing simulation and DFT results for the same choices of $L$, $\\mu$, $T$ and $V_\\mathrm{ext}(x)$, the density profiles of the hard rod fluid are consistent.\n",
    "However, it is obvious that both methods have substantial (and quite contrary) restrictions:\n",
    "- The simulation data is noisy, which can only be improved with longer simulation runs. This quickly becomes prohibitively expensive, in particular if one wishes to perform many individual simulations to explore the behavior for different system parameters. However, one is free to change the type of considered fluid by simply modifying the form of the internal interactions.\n",
    "- The DFT calculation is fast and does not suffer from noisy results, thus enabling vast and efficient parameter studies. The results are exact for the case of hard rods, but as illustrated above, one had to find and implement a suitable density functional in order to capture intrinsic correlations. For more complex fluids, there is little hope in deriving an exact functional analytically, and the construction of good approximations often proves to be very difficult.\n",
    "\n",
    "In the final part of this tutorial, we will combine the advantages of both methods with the help of machine learning.\n",
    "We will demonstrate how to acquire an accurate and flexible representation of $c_1(x; [\\rho])$ by training a neural network with simulation data.\n",
    "The resulting *neural functional* can be used in the minimization scheme as seen above, but it can also reveal more fundamental information about the statistical mechanics of the considered fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b885ad7-9bba-409a-9069-a595801b8568",
   "metadata": {},
   "source": [
    "## Part 3: Neural functional theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7f3c7-fbf2-4fcf-a01b-07c3f4461651",
   "metadata": {},
   "source": [
    "As we have seen in Part 2, classical DFT is conceptually powerful but limited by the difficulty of finding analytic expressions for $c_1(x; [\\rho])$.\n",
    "In the following, we will train a neural network with simulation data (see Part 1) to obtain a representation of $c_1(x; [\\rho])$.\n",
    "\n",
    "To generate a suitable data set, simulations of systems with randomized inhomogeneous external potentials $V_\\mathrm{ext}(x)$ and random values of the chemical potential $\\mu$ are performed.\n",
    "For the construction of an appropriate form of $V_\\mathrm{ext}(x)$, we combine Fourier modes, linear segments and hard walls, which are defined in the following.\n",
    "We also write a method which generates a combination of these elementary functions with randomly chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ec0a2-b2e9-4af2-ac4f-076f74ae7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vext_sin(x; n::Int, A::Number, φ::Number, L::Number) = A * sin(2π * x * n / L + φ)\n",
    "\n",
    "Vext_lin(x; x1::Number, x2::Number, E1::Number, E2::Number) = x > x1 && x < x2 ? E1 + (x - x1) * (E2 - E1) / (x2 - x1) : 0\n",
    "\n",
    "Vext_wall(x; xw::Number, L::Number) = x < xw || x > L - xw ? Inf : 0\n",
    "\n",
    "function generate_Vext(L::Number; num_sin=4, num_lin=rand(1:5), wall=true)\n",
    "    Avar = 1.0\n",
    "    sin_parameters = []\n",
    "    for n in 1:num_sin\n",
    "        push!(sin_parameters, (n = n, A = randn() * Avar, φ = rand() * 2π, L = L))\n",
    "    end\n",
    "    Evar = 1.0\n",
    "    lin_parameters = []\n",
    "    for _ in 1:num_lin\n",
    "        push!(lin_parameters, (x1 = round(rand() * L, digits=2), x2 = round(rand() * L, digits=2), E1 = randn() * Evar, E2 = randn() * Evar))\n",
    "    end\n",
    "    xwmax = 1.0\n",
    "    wall_params = (xw = round(rand() * xwmax, digits=2), L = L)\n",
    "    function (x)\n",
    "        result = 0.0\n",
    "        for sin_params in sin_parameters\n",
    "            result += Vext_sin(x; sin_params...)\n",
    "        end\n",
    "        for lin_params in lin_parameters\n",
    "            result += Vext_lin(x; lin_params...)\n",
    "        end\n",
    "        if wall\n",
    "            result += Vext_wall(x; wall_params...)\n",
    "        end\n",
    "        result\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77eed93-1c98-4e8a-8d1a-d16dc1d60a24",
   "metadata": {},
   "source": [
    "To get an idea of the randomized inhomogeneous environments, let us generate and plot some $V_\\mathrm{ext}(x)$ profiles.\n",
    "Run the following code cell a few times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3d5c0-4343-4bd1-92fa-232aa2311648",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 10.0\n",
    "dx = 0.01\n",
    "xs = dx/2:dx:L\n",
    "Vext_generated = generate_Vext(L)\n",
    "plot(xs, Vext_generated.(xs), label=\"Vext\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20481ad5-30ac-4a47-90dc-2d623d6dd255",
   "metadata": {},
   "source": [
    "We can now proceed to generate reference data by running simulations with these randomized external potentials and random values of the chemical potential as input.\n",
    "Of course, this means spending some computational resources.\n",
    "You can run the simulations yourself, but be aware that this does take a considerable amount of compute time (~hours) if you want to get good data.\n",
    "For this, change `use_prepared_simulations` to `false` in the following code cell and set an appropriate number of simulations and the time to spend for the equilibration and production stage.\n",
    "Alternatively, you could \"fake\" the simulations by using the exact Percus DFT as illustrated in Part 2.\n",
    "\n",
    "For readers without the required computational resources and/or patience, we have prepared a ready-to-use data set for the following machine learning tasks, which is downloaded by default as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a9508-ee8e-4219-bd2c-c08c51b1bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles, Downloads\n",
    "\n",
    "use_prepared_simulations = true  # Choose if you want to use pregenerated simulations (true) or if you want to generate a simulation data set from scratch (false)\n",
    "\n",
    "if use_prepared_simulations\n",
    "    println(\"Downloading pregenerated simulation data set\")\n",
    "    \n",
    "    Downloads.download(\"https://www.staff.uni-bayreuth.de/~bt306964/neuraldft-tutorial/data.tar\", \"data.tar\")\n",
    "    run(`tar xf data.tar`)  # You should now have a directory \"data\" with a bunch of simulation files\n",
    "    datadir = \"data\"\n",
    "else\n",
    "    num_sim = 512\n",
    "    equilibration_time = Dates.Second(10)\n",
    "    production_time = Dates.Second(200)\n",
    "    nthreads = Threads.nthreads()\n",
    "    println(\"Generating simulation reference data from scratch. This will take approximately $(canonicalize(num_sim * (equilibration_time + production_time) / nthreads)).\")\n",
    "    \n",
    "    L = 10\n",
    "    ϕ(r) = r < 1.0 ? Inf : 0  # Hard core repulsion\n",
    "    μmin, μmax = -7.0, 5.0\n",
    "    T = 1.0\n",
    "    \n",
    "    datadir = mkdir(\"data_$(now())\")\n",
    "    println(\"Saving results to $(datadir)\")\n",
    "    Threads.@threads for i in 1:num_sim\n",
    "        μ = μmin + rand() * (μmax - μmin)\n",
    "        Vext_generated = generate_Vext(L)\n",
    "        println(\"Simulation $(i) running...\")\n",
    "        xs, ρ = simulate(L, μ, T, Vext_generated, ϕ; equilibration_time, production_time)\n",
    "        μloc = μ .- Vext_generated.(xs)\n",
    "        writedlm(\"$(datadir)/$(i).dat\", [xs μloc ρ])\n",
    "        println(\"Simulation $(i) done\")\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"There are $(length(readdir(datadir))) result files in the directory $(datadir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88961f1-24b5-4cbf-b70a-2a292c1b875a",
   "metadata": {},
   "source": [
    "If you have run your own simulations above, the results are now located in the directory `data_<timestamp>` (use the file explorer of JupyterLab on the left side).\n",
    "Otherwise, proceed with the pregenerated simulations which are located in `data` after successful download and unpacking.\n",
    "\n",
    "Let us first plot some of the data files to check if they are reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c3412-8a4a-4bf1-94f0-8c238f337bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles, PyPlot\n",
    "\n",
    "datadir = \"data\"  # or use your own simulation results from above\n",
    "\n",
    "sim = rand(readdir(datadir, join=true))  # Select a random simulation within the data directory\n",
    "xs, μloc, ρ = eachcol(readdlm(sim))  # Read the result file and split columns\n",
    "plot(xs, ρ, label=\"ρ\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36a173-95b2-43af-a205-2270cedbd512",
   "metadata": {},
   "source": [
    "Note that besides $\\rho(x)$, we have saved the local chemical potential $\\mu_\\mathrm{loc}(x) = \\mu - V_\\mathrm{ext}(x)$ as a further one-body field.\n",
    "This additional information suffices to calculate the one-body direct correlation function $c_1(x) = \\ln \\rho(x) - \\beta \\mu_\\mathrm{loc}(x)$ for each $x$ where $\\rho(x) \\neq 0$, which is the target of our following investigation.\n",
    "\n",
    "In Part 2, we have shown that the one-body direct correlation function cannot only be obtained pointwise by the above relation, but that it also constitutes a universal functional $c_1(x; [\\rho])$ of the density profile, without having to invoke the local chemical potential $\\mu_\\mathrm{loc}(x)$ explicitly.\n",
    "In the following, we attempt to machine-learn this functional relationship with a neural network.\n",
    "For constructing the specific input-output mapping, we appeal to some physical background.\n",
    "Formally, the functional dependence on $\\rho(x)$ is given with respect to the profile of the whole system.\n",
    "However, for short-ranged pair interactions, the influence of the surrounding density profile on the value of $c_1(x; [\\rho])$ at a certain position $x$ also remains very short-ranged.\n",
    "Therefore, we can adopt a *local* learning strategy: the density profile is taken only within a narrow window around a given position as input to the neural network, which is trained to yield the *scalar value* of $c_1(x; [\\rho])$ at that position.\n",
    "The whole one-body direct correlation profile can be recovered by evaluating the neural network at different positions across the system domain.\n",
    "\n",
    "Therefore, prior to the actual training, the data has to be prepared accordingly, i.e. the input-output pairs of $\\rho$-windows and $c_1$-values have to be generated from the full profiles.\n",
    "For this, some utility functions are given in the file neural.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fed85c-c836-4b9b-a4b7-0fd1dbe8f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"neural.jl\")\n",
    "\n",
    "ρ_profiles, c1_profiles = read_sim_data(datadir)\n",
    "ρ_windows, c1_values = generate_inout(ρ_profiles, c1_profiles; window_bins=201)  # one central bin and 100 bins = 1σ wide to each side\n",
    "\n",
    "size(ρ_windows), size(c1_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccd6cf-9295-4bb3-9f1a-c4dcbe78a97a",
   "metadata": {},
   "source": [
    "The data is now ready to be given to a neural network for training.\n",
    "We choose a simple architecture with fully-connected hidden layers and continuous activation functions (e.g. `softplus`).\n",
    "For the construction of the neural network and the subsequent machine learning routines, the framework [Flux.jl](https://fluxml.ai/) is used.\n",
    "We also specify a standard optimizer and give the generated input-output pairs to a data loader, which automates shuffling and batching during training.\n",
    "As this is a common regression task, we select the mean squared error as the loss function and the mean absolute error as the metric.\n",
    "The learning rate is decreased exponentially after the first few epochs, which improves the final training result.\n",
    "\n",
    "You can choose in the following code cell via `use_pretrained_model` whether you want to download a ready-to-use model or whether you want to do the training yourself (running the training with a GPU is recommended).\n",
    "You can enable GPU support by using the appropriate package for your type of GPU (if you have one), see also https://fluxml.ai/Flux.jl/stable/gpu/ and https://juliagpu.org/.\n",
    "Otherwise, the code runs as-is on the CPU (do not worry about some Info logs which state that CUDA functionality is being called)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5943f3-c73e-4ee3-a48b-11df758c17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON, Dates, Downloads, Flux, Printf\n",
    "using CUDA  # for Nvidia GPU support (you need CUDA.jl and cuDNN.jl). For other GPU manufacturers: AMDGPU, Metal. Comment out if you do not have a GPU\n",
    "\n",
    "use_pretrained_model = true  # Choose if you want to use the pretrained model (true) or if you want to do the training yourself (false)\n",
    "\n",
    "if use_pretrained_model\n",
    "    println(\"Downloading pretrained model\")\n",
    "    \n",
    "    Downloads.download(\"https://www.staff.uni-bayreuth.de/~bt306964/neuraldft-tutorial/model.bson\", \"model.bson\")\n",
    "    BSON.@load \"model.bson\" model\n",
    "else\n",
    "    println(\"Training model from scratch\")\n",
    "    ρ_windows, c1_values = (ρ_windows, c1_values) |> gpu\n",
    "    \n",
    "    model = Chain(\n",
    "        Dense(size(ρ_windows)[1] => 128, softplus),\n",
    "        Dense(128 => 64, softplus),\n",
    "        Dense(64 => 32, softplus),\n",
    "        Dense(32 => 1)\n",
    "    ) |> gpu\n",
    "    \n",
    "    display(model)  # Show a summary of the model with the number of fittable parameters\n",
    "    \n",
    "    opt = Flux.setup(Adam(), model)  # Set up a standard Adam optimizer\n",
    "    \n",
    "    loader = Flux.DataLoader((ρ_windows, c1_values), batchsize=256, shuffle=true)  # Initialize the DataLoader to yield shuffled batches\n",
    "    \n",
    "    loss(m, x, y) = Flux.mse(m(x), y)  # Use mean squared error as loss\n",
    "    metric(m, x, y) = Flux.mae(m(x), y)  # Use mean absolute error as metric\n",
    "    \n",
    "    get_learning_rate(epoch; initial=0.0001, rate=0.03, wait=5) = epoch < wait ? initial : initial * (1 - rate)^(epoch - wait)\n",
    "\n",
    "    model_savefile = \"model_$(now()).bson\"\n",
    "    println(\"Saving model to $(model_savefile)\")\n",
    "    for epoch in 1:250  # Do the training loop\n",
    "        learning_rate = get_learning_rate(epoch)\n",
    "        Flux.adjust!(opt, learning_rate)\n",
    "        @printf \"Epoch: %3i (learning_rate: %.2e)...\" epoch learning_rate; flush(stdout)\n",
    "        Flux.train!(loss, model, loader, opt)\n",
    "        @printf \" loss: %.5f, metric: %.5f\\n\" loss(model, ρ_windows, c1_values) metric(model, ρ_windows, c1_values); flush(stdout)\n",
    "        model = model |> cpu\n",
    "        BSON.@save model_savefile model\n",
    "        model = model |> gpu\n",
    "    end\n",
    "end\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbdbc6-dc0e-4b7b-b097-b1b184a772ba",
   "metadata": {},
   "source": [
    "For convenience, we write a method that lets us evaluate the whole $c_1$-profile via the trained model for a given density profile.\n",
    "This is necessary due to the local nature of the model, which prevents $\\rho(x)$ being used directly as input.\n",
    "Instead, $\\rho(x)$ must be structured into appropriate windows, which are then passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77be17-38a2-43e8-b983-01a9218c5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_c1_neural(model, xs)\n",
    "    window_bins = length(model.layers[1].weight[1,:])\n",
    "    model = model |> gpu\n",
    "    function (ρ)\n",
    "        ρ_windows = generate_windows(ρ; window_bins) |> gpu  # The helper function generate_windows is defined in neural.jl\n",
    "        model(ρ_windows) |> cpu |> vec  # Evaluate the model, make sure the result gets back to the CPU, and transpose it to a vector\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794784ed-6f48-4041-a857-e4758f339982",
   "metadata": {},
   "source": [
    "The central use of the neural functional is revealed when we apply it for the self-consistent calculation of density profiles.\n",
    "Just like we have used the analytical functional due to Percus in Part 2, we can now employ the neural functional to obtain results for $c_1(x; [\\rho])$, which are then used to update and iterate $\\rho(x)$ in the minimization scheme until convergence.\n",
    "Hence, this *neural* DFT is an effective means to circumvent the cumbersome search for analytical functionals.\n",
    "\n",
    "We show in the following that neural DFT indeed yields accurate results for the considered hard rod fluid.\n",
    "Exact reference data for comparison can be obtained immediately by the Percus theory (one would have to resort to simulation results for other fluid models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a13f3f-0a98-4e6b-b3a2-3ab921e4d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 10.0\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0\n",
    "\n",
    "xs, ρ_neural = minimize(L, μ, T, Vext, xs -> get_c1_neural(model, xs))\n",
    "xs, ρ_Percus = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, ρ_neural, label=\"ρ (neural)\"); plot(xs, ρ_Percus, label=\"ρ (Percus)\", linestyle=\"dashed\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d43e0-c55e-4b0e-8f23-ddf6c25dab42",
   "metadata": {},
   "source": [
    "Although the box size has been kept constant during training and in the above example, we can immediately apply the neural functional for different choices of $L$ due to its local nature.\n",
    "This also gives us the possibility to run some rather demanding quality checks.\n",
    "For narrow confinement, which has clearly not been included in our training data, the hard rod fluid yields very interesting density profiles.\n",
    "The following test reveals that the neural functional is able to extrapolate to these unseen cases and that the results from neural DFT match the analytical theory with surprising accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27243c56-f7b7-44ca-bc0c-ff3e1041a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 3.0  # also try other values between 2.0 and 4.0\n",
    "μ, T = 2.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0\n",
    "\n",
    "xs, ρ_neural = minimize(L, μ, T, Vext, xs -> get_c1_neural(model, xs))\n",
    "xs, ρ_Percus = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, ρ_neural, label=\"ρ (neural)\"); plot(xs, ρ_Percus, label=\"ρ (Percus)\", linestyle=\"dashed\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182361d-f3fb-4dec-aca2-3c0d6d2d34db",
   "metadata": {},
   "source": [
    "We can also increase $L$ significantly compared to the previous cases.\n",
    "The ability of the neural functional to be used straightforwardly in this manner opens up the possibility of efficient multiscale investigations.\n",
    "In the following, this is illustrated by considering the sedimentation of the hard rod fluid in a column with hard walls at the top and at the bottom.\n",
    "The external potential is chosen to slowly increase linearly with height $x$ in order to model the effect of gravity.\n",
    "Within the sedimentation column, the density closely follows the equation of state of the hard rod fluid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343c6db-8c21-4491-9038-9d49d848b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 100.0  # you can try to increase L even further\n",
    "μ, T = 0.0, 1.0\n",
    "Vext(x) = x < 0.5 || x > L - 0.5 ? Inf : 0.01 * x  # Hard walls at the boundaries and linearly increasing within the sedimentation column\n",
    "\n",
    "xs, ρ_neural = minimize(L, μ, T, Vext, xs -> get_c1_neural(model, xs))\n",
    "xs, ρ_Percus = minimize(L, μ, T, Vext, get_c1_Percus)\n",
    "\n",
    "plot(xs, ρ_neural, label=\"ρ (neural)\"); plot(xs, ρ_Percus, label=\"ρ (Percus)\", linestyle=\"dashed\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2352da7-65db-471d-a910-545ca48bc610",
   "metadata": {},
   "source": [
    "Besides using the trained model directly in DFT minimizations, we show in the following that one can extract much more fundamental physical information from the neural functional.\n",
    "For this, we aim at implementing functional calculus on the basis of the neural network.\n",
    "An important prerequisite for this is to understand the concept of automatic differentiation.\n",
    "In fact, we have used autodifferentiation already above to realize the adaptation of the neural network parameters during its training.\n",
    "This has been hidden within the `Flux.train!` method, which evaluates for the given training data the output of the neural network as well as gradients with respect to all trainable parameters.\n",
    "The loss function then yields an error to the target output values, which can be traced back to the weights due to the availability of these gradients in order to nudge them in the \"right direction\", i.e. to decrease the loss.\n",
    "\n",
    "Automatic differentiation computes these gradients by recording individual subexpressions of a given composite function, e.g. the neural network, during evaluation.\n",
    "The chain rule is then applied programmatically to evaluate derivatives of the function.\n",
    "As this mechanism is central to machine learning, many frameworks come with ready-to-use implementations of automatic differentiation.\n",
    "Flux.jl provides the `gradient` function, which is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7929-21b4-4c2f-824c-c4c71661d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "\n",
    "f(x, y) = x^2 + sin(x * y)  # Some test function\n",
    "df(x, y) = (2*x + y * cos(x * y), x * cos(x * y))  # Analytic derivative to test autodiff results\n",
    "\n",
    "x, y = π, 2.0\n",
    "\n",
    "println(\"autodiff: \", gradient(f, x, y), \", analytic: \", df(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85ea55-833d-4134-b286-e920770ad897",
   "metadata": {},
   "source": [
    "We can now easily and efficiently derive the output of our neural functional with respect to the input density.\n",
    "But what do we get out of this?\n",
    "\n",
    "In Part 2, we have seen that the one-body direct correlation functional is obtained from the functional derivative of the excess free energy, $c_1(x; [\\rho]) = -\\delta \\beta F_\\mathrm{exc}[\\rho] / \\delta \\rho(x)$, and that it is a central object in DFT calculations.\n",
    "Applying an additional functional derivative yields another important quantity, the two-body direct correlation function \n",
    "$$\n",
    "c_2(x, x'; [\\rho]) = \\frac{\\delta c_1(x; [\\rho])}{\\delta \\rho(x')}.\n",
    "$$\n",
    "In fact, a complete hierarchy of direct correlation functions $c_n(x, x', ...; [\\rho])$ is defined by iterating more functional derivatives.\n",
    "\n",
    "From fundamental physical considerations, further relations can be obtained for these direct correlation functions.\n",
    "In particular, by applying Noether's theorem to statistical mechanical systems, one arrives at so-called sum rules, which constrain the interrelation of different quantities of interest.\n",
    "One such sum rule which connects $c_1(x)$, $c_2(x, x')$ and $\\rho(x)$ reads as follows:\n",
    "$$\n",
    "\\nabla c_1(x) = \\int \\mathrm{d}x' c_2(x, x') \\nabla' \\rho(x').\n",
    "$$\n",
    "As these identities emerge from fundamental invariances of equilibrium many-body systems, they can be used as further valuable tests to check the consistency of the neural functional.\n",
    "\n",
    "We implement in the following a method to calculate $c_2(x, x'; [\\rho])$ as obtained via autodifferentiation of a given function for $c_1(x; [\\rho])$.\n",
    "Note that autodifferentiation can be used very generically, i.e. this gives us a way to obtain $c_2(x, x'; [\\rho])$ also from the Percus $c_1(x; [\\rho])$ without having to derive and implement the analytic expression ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae80ea5-ab40-41be-83df-7f0dd6ec7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_c2_autodiff(c1_function, xs)\n",
    "    dx = xs[2] - xs[1]\n",
    "    function (ρ)\n",
    "        Flux.jacobian(c1_function, ρ)[1] / dx\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbb52c-b91a-4ede-baa1-c387b69298c6",
   "metadata": {},
   "source": [
    "First, we run some quick tests to illustrate the usage.\n",
    "As test density input, we take some analytic results that follow from a randomized continuous external potential.\n",
    "Run the cell a few times to see different results for $c_2(x, x'; [\\rho])$.\n",
    "Also try to switch out the neural correlation functional for the Percus expression, the use of autodifferentiation remains straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9160f3-cb34-4f63-a404-c2be8e2556ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 10.0\n",
    "μ, T = 0.0, 1.0\n",
    "\n",
    "xs, ρ = minimize(L, μ, T, generate_Vext(L; num_sin=4, num_lin=0, wall=false), get_c1_Percus)\n",
    "\n",
    "c1_func = get_c1_neural(model, xs)\n",
    "# c1_func = get_c1_Percus(xs)  # the Percus c1 can also be used in the following autodifferentiation, no need for pen-and-paper derivation :)\n",
    "\n",
    "c2_func = get_c2_autodiff(c1_func, xs)\n",
    "\n",
    "c1 = c1_func(ρ)\n",
    "c2 = c2_func(ρ)\n",
    "\n",
    "imshow(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19e9dd-d363-48e4-86d3-3187a5b2a23f",
   "metadata": {},
   "source": [
    "Now we can check the above Noether sum rule.\n",
    "The gradients are evaluated numerically via finite differences for which we have also supplied a function `finite_diff` in neural.jl.\n",
    "The following plot shows the right and left hand side of the identity, which coincide when evaluating the quantities with a neural functional that has been trained sufficiently well.\n",
    "Recall that we have neither given $c_2(x, x'; [\\rho])$ nor the Noether identity during training.\n",
    "The successful reproduction of such fundamental results implies that the neural functional does not merely interpolate values of $c_1(x, [\\rho])$, but that it instead also captures the essential physics of the underlying many-body problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea1349-40ac-4308-a078-1fcd24897d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = xs[2] - xs[1]\n",
    "grad_c1 = finite_diff(c1; dx)\n",
    "grad_ρ = finite_diff(ρ; dx)\n",
    "rhs = c2 * grad_ρ * dx\n",
    "plot(grad_c1, label=\"∇c1(x)\"); plot(rhs, linestyle=\"dashed\", label=\"∫dx' c2(x,x') ∇'ρ(x')\"); legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc1c45-d708-4760-9788-d6937dfc0d52",
   "metadata": {},
   "source": [
    "To complete the functional calculus, we now consider the *inverse* of functional differentiation.\n",
    "The availability of such an operation gives us direct access to the excess free energy $F_\\mathrm{exc}[\\rho]$, as can be deduced from the definition of $c_1(x; [\\rho])$.\n",
    "Formally, one obtains a useful expression for this task by considering functional line integration, which amounts to performing a line integral in function space.\n",
    "By choosing a linear parameterization $\\rho_a(x) = a \\rho(x)$, one can derive the explicit result\n",
    "$$\n",
    "\\beta F_\\mathrm{exc}[\\rho] = - \\int \\mathrm{d}x \\rho(x) \\int_0^1 \\mathrm{d}a c_1(x, [\\rho_a]).\n",
    "$$\n",
    "\n",
    "On the level of the neural functional (or, in fact, any representation of the one-body direct correlation functional), this expression is easy to evaluate numerically by discretization of the $a$-integral.\n",
    "We write a method for this in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab99b01-739a-4b5f-8f38-db0ddb645c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_Fexc_funcintegral(c1_function, xs; num_a=30)\n",
    "    dx = xs[2] - xs[1]\n",
    "    da = 1 / num_a\n",
    "    as = da/2:da:1\n",
    "    function (ρ)\n",
    "        aintegral = zero(ρ)\n",
    "        for a in as\n",
    "            aintegral .+= c1_function(a .* ρ)\n",
    "        end\n",
    "        -sum(ρ .* aintegral) * dx * da\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b57164-90a3-4bef-b471-ec35ae5a9af0",
   "metadata": {},
   "source": [
    "The functional line integration is tested as above for randomized inhomogeneous environments.\n",
    "We also add an explicit consistency check to the analytic Percus free energy functional.\n",
    "The values of $F_\\mathrm{exc}[\\rho]$ as obtained by functional line integration of the neural correlation functional show almost no discrepancy to the exact analytic theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af931ca-2218-425d-8de2-50598c4e2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "L = 10.0\n",
    "μ, T = 0.0, 1.0\n",
    "\n",
    "xs, ρ = minimize(L, μ, T, generate_Vext(L), get_c1_Percus)\n",
    "\n",
    "c1_func = get_c1_neural(model, xs)\n",
    "# c1_func = get_c1_Percus(xs)\n",
    "\n",
    "Fexc_func = get_Fexc_funcintegral(c1_func, xs)\n",
    "\n",
    "function get_Fexc_Percus(xs)\n",
    "    dx = xs[2] - xs[1]\n",
    "    ω0, ω1 = get_weights_Percus(xs)\n",
    "    conv(f, g) = conv_fft(f, g; dx)\n",
    "    function (ρ)\n",
    "        n0, n1 = conv(ω0, ρ), conv(ω1, ρ)\n",
    "        ϕ = n0 .* log.(1 .- n1)\n",
    "        -sum(ϕ) * dx\n",
    "    end\n",
    "end\n",
    "\n",
    "Fexc_Percus = get_Fexc_Percus(xs)\n",
    "\n",
    "println(\"Fexc from analytic expression (Percus): \", Fexc_Percus(ρ))\n",
    "println(\"Fexc from functional line integral (neural): \", Fexc_func(ρ))\n",
    "plot(ρ, label=\"ρ\"); legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
